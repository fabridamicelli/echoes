{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"echoes \u00b6 High level API for machine learning with Echo State Networks (ESN) \u2013 work in progress!. Check out the examples for a quick start and What are Echo State Networks? section for a little intro about Echo State Networks. The library is scikit-learn compatible, thus you can directly use sklearn utils, such as GridSearchCV . Moreover, it tries to stick to the intuitions that you might bring from using sklearn. For example, models can be initialized without passing arguments (but kwargs are enforced if you choose to pass any); attributes generated during fitting are stored with trailing underscore; etc. Installation \u00b6 You can install the package via pip pip install echoes Citing \u00b6 If you find echoes useful for a publication, then please use the following BibTeX to cite it: @misc { echoes , author = { Damicelli , Fabrizio } , title = { echoes : Echo State Networks with Python } , year = { 2019 } , publisher = { GitHub } , journal = { GitHub repository } , howpublished = {\\ url { https : // github . com / fabridamicelli / echoes }} , } Requirements \u00b6 Dependencies \u00b6 numpy numba scikit-learn pandas matplotlib seaborn tests: mypy, pytest The code has been tested with Python 3.7 on Ubuntu 16.04/18.04. Tests \u00b6 Run tests with make test","title":"Home"},{"location":"#echoes","text":"High level API for machine learning with Echo State Networks (ESN) \u2013 work in progress!. Check out the examples for a quick start and What are Echo State Networks? section for a little intro about Echo State Networks. The library is scikit-learn compatible, thus you can directly use sklearn utils, such as GridSearchCV . Moreover, it tries to stick to the intuitions that you might bring from using sklearn. For example, models can be initialized without passing arguments (but kwargs are enforced if you choose to pass any); attributes generated during fitting are stored with trailing underscore; etc.","title":"echoes"},{"location":"#installation","text":"You can install the package via pip pip install echoes","title":"Installation"},{"location":"#citing","text":"If you find echoes useful for a publication, then please use the following BibTeX to cite it: @misc { echoes , author = { Damicelli , Fabrizio } , title = { echoes : Echo State Networks with Python } , year = { 2019 } , publisher = { GitHub } , journal = { GitHub repository } , howpublished = {\\ url { https : // github . com / fabridamicelli / echoes }} , }","title":"Citing"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#dependencies","text":"numpy numba scikit-learn pandas matplotlib seaborn tests: mypy, pytest The code has been tested with Python 3.7 on Ubuntu 16.04/18.04.","title":"Dependencies"},{"location":"#tests","text":"Run tests with make test","title":"Tests"},{"location":"api/ESNGenerator/","text":"The number of inputs (n_inputs) is always 1 and n_outputs is infered from passed data. It uses always feedback, so that is not a parameter anymore (always True). Parameters: Name Type Description Default n_steps int, default=100 Number of steps to generate pattern (used by predict method). required n_reservoir int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed. required W np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed. required spectral_radius float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W). required W_in np.ndarray of shape (n_reservoir, 1+n_inputs) (1->bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used. required W_fb np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback. required sparsity float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored. required noise float, optional, default=0 Scaling factor of the (uniform) noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5. required leak_rate float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage. required bias int, float or np.ndarray, optional, default=1 Value of the bias neuron, injected at each time to the reservoir neurons. If int or float, all neurons receive the same. If np.ndarray is must be of length n_reservoir. required activation function (numba jitted), optional, default=tanh Non-linear activation function applied to the neurons at each step. For numba acceleration, it must be a jitted function. Basic activation functions as tanh, sigmoid, relu are already available in echoe.utils. Either use those or write a custom one decorated with numba njit. required activation_out function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed. required fit_only_states bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase. required regression_method str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used. required ridge_alpha float, ndarray of shape (n_outputs,), default=None Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details. required ridge_fit_intercept bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details. required ridge_normalize bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. See sklearn Ridge documentation for details. required ridge_max_iter int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details. required ridge_tol float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details. required ridge_solver str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details. required ridge_sample_weight float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details. required n_transient int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak. required random_state int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . required store_states_train bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_. required store_states_pred bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_. required Attributes: \u00b6 - W_out_ : array of shape ( n_outputs , n_inputs + n_reservoir + 1 ). Outgoing weights after fitting linear regression model to predict outputs . - training_prediction_: array of shape ( n_samples , n_outputs ). Predicted output on training data . - states_train_: array of shape ( n_samples , n_reservoir ), default False . If store_states_train is True , states matrix is stored for visualizing reservoir neurons activity during training . - states_pred_: array of shape ( n_samples , n_reservoir ), default False . If store_states_pred is True , states matrix is stored for visualizing reservoir neurons activity during prediction ( test ). fit ( self , X = None , y = None ) \u00b6 Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. None Returns: Type Description self returns an instance of self. Source code in echoes/esn/_generator.py def fit ( self , X = None , y = None ) -> \"ESNGenerator\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. Returns: self: returns an instance of self. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator only takes y for training\" ) y = check_array ( y , ensure_2d = False , dtype = np . float64 ) self . _dtype_ = y . dtype if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) outputs = y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) # Pattern generation takes no input, thus hardcode for later # construction of matrices self . n_inputs_ = 1 self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) # Make inputs zero inputs = np . zeros ( shape = ( outputs . shape [ 0 ], self . n_inputs_ ), dtype = self . _dtype_ ) check_consistent_length ( inputs , outputs ) # sanity check # Initialize reservoir model self . reservoir_ = self . _init_reservoir_neurons () states = self . reservoir_ . harvest_states ( inputs , outputs , initial_state = None ) # Extend states matrix with inputs; i.e., make [h(t); x(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (including the pass through the output nonlinearity) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Keep last state for later self . last_state_ = states [ - 1 , :] self . last_input_ = inputs [ - 1 , :] self . last_output_ = outputs [ - 1 , :] # Store reservoir activity if self . store_states_train : self . states_train_ = states return self predict ( self , X = None ) \u00b6 Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Parameters: Name Type Description Default X None, always ignored, API consistency None Returns: Type Description outputs 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. Source code in echoes/esn/_generator.py def predict ( self , X = None ) -> np . ndarray : \"\"\" Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Arguments: X: None, always ignored, API consistency Returns: outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator takes no X for prediction\" ) assert self . n_steps >= 1 , \"n_steps must be >= 1\" n_steps = self . n_steps # shorthand # Initialize predictions: begin with last state as first state inputs = np . zeros ( shape = ( n_steps , self . n_inputs_ ), dtype = self . _dtype_ ) inputs = np . vstack ([ self . last_input_ , inputs ]) states = np . vstack ([ self . last_state_ , np . zeros (( n_steps , self . n_reservoir_ ), dtype = self . _dtype_ ) ]) outputs = np . vstack ([ self . last_output_ , np . zeros (( n_steps , self . n_outputs_ ), dtype = self . _dtype_ ) ]) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for t in range ( 1 , outputs . shape [ 0 ]): states [ t , :] = self . reservoir_ . update_state ( state_t = states [ t - 1 , :], X_t = inputs [ t , :], y_t = outputs [ t - 1 , :], ) if self . fit_only_states : full_states = states [ t , :] else : full_states = np . concatenate ([ states [ t , :], inputs [ t , :]]) # Predict outputs [ t , :] = self . W_out_ @ full_states # TODO: Update last_{input, states, outputs}_ # Store reservoir activity if self . store_states_pred : self . states_pred_ = states [ 1 :, :] # discard first step (comes from fitting) # Apply output non-linearity outputs = self . activation_out ( outputs ) return outputs [ 1 :, :] # discard initial step (comes from training) score ( self , X = None , y = None , sample_weight = None ) \u00b6 Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. None y 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. None Returns: Type Description score float R2 score Source code in echoes/esn/_generator.py def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. Returns: score: float R2 score \"\"\" return r2_score ( y , self . predict (), sample_weight = sample_weight )","title":"ESNGenerator"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator--attributes","text":"- W_out_ : array of shape ( n_outputs , n_inputs + n_reservoir + 1 ). Outgoing weights after fitting linear regression model to predict outputs . - training_prediction_: array of shape ( n_samples , n_outputs ). Predicted output on training data . - states_train_: array of shape ( n_samples , n_reservoir ), default False . If store_states_train is True , states matrix is stored for visualizing reservoir neurons activity during training . - states_pred_: array of shape ( n_samples , n_reservoir ), default False . If store_states_pred is True , states matrix is stored for visualizing reservoir neurons activity during prediction ( test ).","title":"Attributes:"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.fit","text":"Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. None Returns: Type Description self returns an instance of self. Source code in echoes/esn/_generator.py def fit ( self , X = None , y = None ) -> \"ESNGenerator\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. Returns: self: returns an instance of self. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator only takes y for training\" ) y = check_array ( y , ensure_2d = False , dtype = np . float64 ) self . _dtype_ = y . dtype if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) outputs = y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) # Pattern generation takes no input, thus hardcode for later # construction of matrices self . n_inputs_ = 1 self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) # Make inputs zero inputs = np . zeros ( shape = ( outputs . shape [ 0 ], self . n_inputs_ ), dtype = self . _dtype_ ) check_consistent_length ( inputs , outputs ) # sanity check # Initialize reservoir model self . reservoir_ = self . _init_reservoir_neurons () states = self . reservoir_ . harvest_states ( inputs , outputs , initial_state = None ) # Extend states matrix with inputs; i.e., make [h(t); x(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (including the pass through the output nonlinearity) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Keep last state for later self . last_state_ = states [ - 1 , :] self . last_input_ = inputs [ - 1 , :] self . last_output_ = outputs [ - 1 , :] # Store reservoir activity if self . store_states_train : self . states_train_ = states return self","title":"fit()"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.predict","text":"Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Parameters: Name Type Description Default X None, always ignored, API consistency None Returns: Type Description outputs 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. Source code in echoes/esn/_generator.py def predict ( self , X = None ) -> np . ndarray : \"\"\" Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Arguments: X: None, always ignored, API consistency Returns: outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator takes no X for prediction\" ) assert self . n_steps >= 1 , \"n_steps must be >= 1\" n_steps = self . n_steps # shorthand # Initialize predictions: begin with last state as first state inputs = np . zeros ( shape = ( n_steps , self . n_inputs_ ), dtype = self . _dtype_ ) inputs = np . vstack ([ self . last_input_ , inputs ]) states = np . vstack ([ self . last_state_ , np . zeros (( n_steps , self . n_reservoir_ ), dtype = self . _dtype_ ) ]) outputs = np . vstack ([ self . last_output_ , np . zeros (( n_steps , self . n_outputs_ ), dtype = self . _dtype_ ) ]) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for t in range ( 1 , outputs . shape [ 0 ]): states [ t , :] = self . reservoir_ . update_state ( state_t = states [ t - 1 , :], X_t = inputs [ t , :], y_t = outputs [ t - 1 , :], ) if self . fit_only_states : full_states = states [ t , :] else : full_states = np . concatenate ([ states [ t , :], inputs [ t , :]]) # Predict outputs [ t , :] = self . W_out_ @ full_states # TODO: Update last_{input, states, outputs}_ # Store reservoir activity if self . store_states_pred : self . states_pred_ = states [ 1 :, :] # discard first step (comes from fitting) # Apply output non-linearity outputs = self . activation_out ( outputs ) return outputs [ 1 :, :] # discard initial step (comes from training)","title":"predict()"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.score","text":"Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. None y 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. None Returns: Type Description score float R2 score Source code in echoes/esn/_generator.py def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. Returns: score: float R2 score \"\"\" return r2_score ( y , self . predict (), sample_weight = sample_weight )","title":"score()"},{"location":"api/ESNRegressor/","text":"Number of input and output neurons are infered from passed data. Parameters: Name Type Description Default n_reservoir int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed. required W np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed. required spectral_radius float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W). required W_in np.ndarray of shape (n_reservoir, 1+n_inputs) (1->bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used. required W_fb np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback. required sparsity float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored. required noise float, optional, default=0 Scaling factor of the (uniform) noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5. required leak_rate float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage. required bias int, float or np.ndarray, optional, default=1 Value of the bias neuron, injected at each time to the reservoir neurons. If int or float, all neurons receive the same. If np.ndarray is must be of length n_reservoir. required input_scaling float or np.ndarray of length n_inputs, default=None Scalar to multiply each input before feeding it to the network. If float, all inputs get multiplied by same value. If array, it must match n_inputs length (X.shape[1]), specifying the scaling factor for each input. required input_shift float or np.ndarray of length n_inputs, default=None Scalar to add to each input before feeding it to the network. If float, multiplied same value is added to all inputs. If array, it must match n_inputs length (X.shape[1]), specifying the value to add to each input. required feedback bool, optional, default=False If True, the reservoir also receives the outout signal as input. required activation function (numba jitted), optional, default=tanh Non-linear activation function applied to the neurons at each step. For numba acceleration, it must be a jitted function. Basic activation functions as tanh, sigmoid, relu are already available in echoe.utils. Either use those or write a custom one decorated with numba njit. required activation_out function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed. required fit_only_states bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase. required regression_method str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used. required ridge_alpha float, ndarray of shape (n_outputs,), default=1 Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details. required ridge_fit_intercept bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details. required ridge_normalize bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. See sklearn Ridge documentation for details. required ridge_max_iter int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details. required ridge_tol float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details. required ridge_solver str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details. required ridge_sample_weight float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details. required n_transient int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak. required random_state int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . required store_states_train bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_. required store_states_pred bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_. required Attributes: \u00b6 - W_out_ : array of shape ( n_outputs , n_inputs + n_reservoir + 1 ) Outgoing weights after fitting linear regression model to predict outputs . - training_prediction_: array of shape ( n_samples , n_outputs ) Predicted output on training data . - states_train_: array of shape ( n_samples , n_reservoir ), default False . If store_states_train is True , states matrix is stored for visualizing reservoir neurons activity during training . - states_pred_: array of shape ( n_samples , n_reservoir ), default False . If store_states_pred is True , states matrix is stored for visualizing reservoir neurons activity during prediction ( test ). fit ( self , X , y ) \u00b6 Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X ndarray None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. required y ndarray 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. required Returns self: returns an instance of self. Source code in echoes/esn/_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ) -> \"ESNRegressor\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. Returns self: returns an instance of self. \"\"\" self . _dtype_ = X . dtype X , y = check_X_y ( X , y , multi_output = True , dtype = self . _dtype_ ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) # Check y again (enforcing 2D for multiple outputs) y = check_array ( y , dtype = self . _dtype_ ) # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) self . n_inputs_ = X . shape [ 1 ] self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = y . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) X = self . _scale_shift_inputs ( X ) # Initialize reservoir model self . reservoir_ = self . _init_reservoir_neurons () # Run \"neuronal activity\" states = self . reservoir_ . harvest_states ( X , y , initial_state = None ) # Extend states matrix with inputs, except we only train based on states full_states = states if self . fit_only_states else np . hstack (( states , X )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], y [ self . n_transient :, :] ) # Predict on training set (including the pass through the output nonlinearity) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Store reservoir activity if self . store_states_train : self . states_train_ = states return self predict ( self , X ) \u00b6 Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Parameters: Name Type Description Default X ndarray 2D np.ndarray of shape (n_samples, n_inputs) Input, i.e., X, the features. required Returns: Type Description y_pred 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. Source code in echoes/esn/_regressor.py def predict ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Input, i.e., X, the features. Returns: y_pred: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. \"\"\" check_is_fitted ( self ) X = check_array ( X , dtype = self . _dtype_ ) n_time_steps = X . shape [ 0 ] # Scale and shift inputs X = self . _scale_shift_inputs ( X ) # Initialize predictions states = np . zeros (( n_time_steps , self . n_reservoir_ ), dtype = self . _dtype_ ) y_pred = np . zeros (( n_time_steps , self . n_outputs_ ), dtype = self . _dtype_ ) check_consistent_length ( X , y_pred ) # sanity check # Go through samples (steps) and predict for each of them for t in range ( 1 , n_time_steps ): states [ t , :] = self . reservoir_ . update_state ( state_t = states [ t - 1 , :], X_t = X [ t , :], y_t = y_pred [ t - 1 , :], ) if self . fit_only_states : full_states = states [ t , :] else : full_states = np . concatenate ([ states [ t , :], X [ t , :]]) # Predict y_pred [ t , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states # Apply output non-linearity return self . activation_out ( y_pred ) score ( self , X = None , y = None , sample_weight = None ) \u00b6 R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily bad). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X 2D np.ndarray of shape (n_samples, n_inputs) Test samples. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Usage n_steps_to_remove = 10 weights = np.ones(y_true.shape[0]) weights[: n_steps_to_remove] = 0 score(X, y_true, sample_weight=weights) None Returns: Type Description score float R2 score Source code in echoes/esn/_regressor.py def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily bad). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Test samples. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. **Usage** >> n_steps_to_remove = 10 >> weights = np.ones(y_true.shape[0]) >> weights[: n_steps_to_remove] = 0 >> score(X, y_true, sample_weight=weights) Returns: score: float R2 score \"\"\" y_pred = self . predict ( X ) # If no sample_weight passed, compute the score without considering transient if sample_weight is None : weights = np . ones ( y . shape [ 0 ]) weights [: self . n_transient ] = 0 return r2_score ( y , y_pred , sample_weight = weights ) return r2_score ( y , y_pred , sample_weight = sample_weight )","title":"ESNRegressor"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor--attributes","text":"- W_out_ : array of shape ( n_outputs , n_inputs + n_reservoir + 1 ) Outgoing weights after fitting linear regression model to predict outputs . - training_prediction_: array of shape ( n_samples , n_outputs ) Predicted output on training data . - states_train_: array of shape ( n_samples , n_reservoir ), default False . If store_states_train is True , states matrix is stored for visualizing reservoir neurons activity during training . - states_pred_: array of shape ( n_samples , n_reservoir ), default False . If store_states_pred is True , states matrix is stored for visualizing reservoir neurons activity during prediction ( test ).","title":"Attributes:"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.fit","text":"Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X ndarray None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. required y ndarray 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. required Returns self: returns an instance of self. Source code in echoes/esn/_regressor.py def fit ( self , X : np . ndarray , y : np . ndarray ) -> \"ESNRegressor\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. Returns self: returns an instance of self. \"\"\" self . _dtype_ = X . dtype X , y = check_X_y ( X , y , multi_output = True , dtype = self . _dtype_ ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) # Check y again (enforcing 2D for multiple outputs) y = check_array ( y , dtype = self . _dtype_ ) # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) self . n_inputs_ = X . shape [ 1 ] self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = y . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) X = self . _scale_shift_inputs ( X ) # Initialize reservoir model self . reservoir_ = self . _init_reservoir_neurons () # Run \"neuronal activity\" states = self . reservoir_ . harvest_states ( X , y , initial_state = None ) # Extend states matrix with inputs, except we only train based on states full_states = states if self . fit_only_states else np . hstack (( states , X )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], y [ self . n_transient :, :] ) # Predict on training set (including the pass through the output nonlinearity) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Store reservoir activity if self . store_states_train : self . states_train_ = states return self","title":"fit()"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.predict","text":"Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Parameters: Name Type Description Default X ndarray 2D np.ndarray of shape (n_samples, n_inputs) Input, i.e., X, the features. required Returns: Type Description y_pred 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. Source code in echoes/esn/_regressor.py def predict ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Input, i.e., X, the features. Returns: y_pred: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. \"\"\" check_is_fitted ( self ) X = check_array ( X , dtype = self . _dtype_ ) n_time_steps = X . shape [ 0 ] # Scale and shift inputs X = self . _scale_shift_inputs ( X ) # Initialize predictions states = np . zeros (( n_time_steps , self . n_reservoir_ ), dtype = self . _dtype_ ) y_pred = np . zeros (( n_time_steps , self . n_outputs_ ), dtype = self . _dtype_ ) check_consistent_length ( X , y_pred ) # sanity check # Go through samples (steps) and predict for each of them for t in range ( 1 , n_time_steps ): states [ t , :] = self . reservoir_ . update_state ( state_t = states [ t - 1 , :], X_t = X [ t , :], y_t = y_pred [ t - 1 , :], ) if self . fit_only_states : full_states = states [ t , :] else : full_states = np . concatenate ([ states [ t , :], X [ t , :]]) # Predict y_pred [ t , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states # Apply output non-linearity return self . activation_out ( y_pred )","title":"predict()"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.score","text":"R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily bad). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X 2D np.ndarray of shape (n_samples, n_inputs) Test samples. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Usage n_steps_to_remove = 10 weights = np.ones(y_true.shape[0]) weights[: n_steps_to_remove] = 0 score(X, y_true, sample_weight=weights) None Returns: Type Description score float R2 score Source code in echoes/esn/_regressor.py def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily bad). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Test samples. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. **Usage** >> n_steps_to_remove = 10 >> weights = np.ones(y_true.shape[0]) >> weights[: n_steps_to_remove] = 0 >> score(X, y_true, sample_weight=weights) Returns: score: float R2 score \"\"\" y_pred = self . predict ( X ) # If no sample_weight passed, compute the score without considering transient if sample_weight is None : weights = np . ones ( y . shape [ 0 ]) weights [: self . n_transient ] = 0 return r2_score ( y , y_pred , sample_weight = weights ) return r2_score ( y , y_pred , sample_weight = sample_weight )","title":"score()"},{"location":"api/plotting/","text":"Plotting functions often needed. Not extremely well polished, rather a tool for quick visualization. plot_predicted_ts ( ts_true , ts_pred , start = None , end = None , ax = None , title = '' , figsize = ( 6 , 2 ), legend = True ) \u00b6 Parameters: Name Type Description Default ts_true Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Target time series. required ts_pred Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Predicted time series. required start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None ax Axes plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None None title str str,optional Plot title. '' figsize Tuple tuple Figure size. Default (6, 2). (6, 2) legend bool bool If True, legend is added (\"target\", \"predicted\"). True Returns: Type Description ax matplotlib Axes Returns the Axes object with the plot drawn onto it. Source code in echoes/plotting/_core.py def plot_predicted_ts ( ts_true : Union [ np . ndarray , List , pd . Series ], ts_pred : Union [ np . ndarray , List , pd . Series ], start : int = None , end : int = None , ax : plt . Axes = None , title : str = \"\" , figsize : Tuple = ( 6 , 2 ), legend : bool = True , ) -> None : \"\"\" Arguments: ts_true: np.ndarray, List, pd.Series Target time series. ts_pred: np.ndarray, List, pd.Series Predicted time series. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. ax: plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None title: str,optional Plot title. figsize: tuple Figure size. Default (6, 2). legend: bool If True, legend is added (\"target\", \"predicted\"). Returns: ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. \"\"\" if isinstance ( ts_true , pd . Series ): ts_true = ts_true . values if isinstance ( ts_pred , pd . Series ): ts_pred = ts_pred . values if ax is None : fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) ax . plot ( ts_true [ start : end ], color = \"steelblue\" , label = \"target\" , linewidth = 5.5 ) ax . set_xlabel ( \"time\" ) ax . plot ( ts_pred [ start : end ], linestyle = \"--\" , color = \"orange\" , linewidth = 2 , label = \"prediction\" , ) ax . set_ylabel ( \"output value\" ) ax . set_xlabel ( \"time\" ) if legend : ax . legend () return ax plot_reservoir_activity ( esn , neurons , train = False , pred = True , start = None , end = None , figsize = ( 15 , 9 ), ** kwargs ) \u00b6 Plot the activity, ie time series of states, of the reservoir neurons. Parameters: Name Type Description Default esn Union[echoes.esn._regressor.ESNRegressor, echoes.esn._generator.ESNGenerator] ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. required neurons Union[numpy.ndarray, List] np.ndarray or List List of reservoir neurons indices whose time series will be plotted. required train bool bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. False pred bool bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. True start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None suptitle str, optional Plot suptitle. required figsize Tuple tuple Figure size. Default (15, 10). (15, 9) kwargs dict Plotting kwargs passed to plt.plot {} Returns: Type Description fig plt.figure Figure object for fine tuning. Source code in echoes/plotting/_core.py def plot_reservoir_activity ( esn : Union [ ESNRegressor , ESNGenerator ], neurons : Union [ np . ndarray , List ], train : bool = False , pred : bool = True , start : int = None , end : int = None , figsize : Tuple = ( 15 , 9 ), ** kwargs , ): \"\"\" Plot the activity, ie time series of states, of the reservoir neurons. Arguments: esn: ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. neurons: np.ndarray or List List of reservoir neurons indices whose time series will be plotted. train: bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. pred: bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. suptitle: str, optional Plot suptitle. figsize: tuple Figure size. Default (15, 10). kwargs: dict Plotting kwargs passed to plt.plot Returns: fig: plt.figure Figure object for fine tuning. \"\"\" assert train or pred , \"either train or pred must be True\" assert not ( train and pred ), \"only one of train or pred can be True\" n_neurons = len ( neurons ) # Grab time series to plot ts = esn . states_pred_ if pred else esn . states_train_ # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_neurons / 3 )), ncols = 3 , figsize = figsize ) if \"linewidth\" in kwargs : linewidth = kwargs . pop ( \"linewidht\" ) else : linewidth = 3 if \"color\" in kwargs : color = kwargs . pop ( \"color\" ) else : color = \".6\" for neuron_idx , neuron in enumerate ( neurons ): ax = axes . flat [ neuron_idx ] ax . plot ( ts [ start : end , neuron ], linewidth = linewidth , color = color , ** kwargs ) ax . set_ylabel ( \"state\" ) ax . set_xlabel ( \"time\" ) ax . set_title ( f \"reservoir neuron idx: { neuron } \" ) # Delete unnecessary axes if n_neurons % 3 == 1 : fig . delaxes ( axes . flat [ - 1 ]) fig . delaxes ( axes . flat [ - 2 ]) elif n_neurons % 3 == 2 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout () return fig set_mystyle () \u00b6 Set context and a couple of defaults for nicer plots. Source code in echoes/plotting/_core.py def set_mystyle (): \"\"\"Set context and a couple of defaults for nicer plots.\"\"\" sns . set ( context = \"paper\" , style = \"whitegrid\" , font_scale = 1.4 , rc = { \"grid.linestyle\" : \"--\" , \"grid.linewidth\" : 0.8 }, ) Plotting functions related to the Memory Capacity task. plot_forgetting_curve ( lags , forgetting_curve , ax = None , ** kwargs ) \u00b6 Plot forgetting curve, ie, memory capacity (MC) vs lag. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Sequence of lags used in the memory capacity task. required forgetting_curve ndarray np.ndarray Sequence of results from the memory task. required ax Axes plt.Axes, optional If given plot will use this axes. None kwargs mapping, optional Plotting args passed to ax.plot. {} Source code in echoes/plotting/_memory_capacity.py def plot_forgetting_curve ( lags : Union [ List , np . ndarray ], forgetting_curve : np . ndarray , ax : plt . Axes = None , ** kwargs , ) -> None : \"\"\" Plot forgetting curve, ie, memory capacity (MC) vs lag. Arguments: lags: np.ndarray or List Sequence of lags used in the memory capacity task. forgetting_curve: np.ndarray Sequence of results from the memory task. ax: plt.Axes, optional If given plot will use this axes. kwargs: mapping, optional Plotting args passed to ax.plot. \"\"\" if ax is None : fig , ax = plt . subplots () ax . plot ( lags , forgetting_curve , ** kwargs ) ax . set_xlabel ( \"$k$\" ) ax . set_ylabel ( r \"$MC_k$\" ) plot_mc_predicted_ts ( lags , outputs_true , outputs_pred , start = None , end = None ) \u00b6 Plot true and predicted time series coming from memory capacity task for all lags. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). required ouputs_true np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. required ouputs_pred np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. required start/end int, optional Plot will we timeseries[start: end], to exclude transient. required Source code in echoes/plotting/_memory_capacity.py def plot_mc_predicted_ts ( lags : Union [ List , np . ndarray ], outputs_true : np . ndarray , outputs_pred : np . ndarray , start : int = None , end : int = None , ) -> None : \"\"\" Plot true and predicted time series coming from memory capacity task for all lags. Arguments: lags: np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). ouputs_true: np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. ouputs_pred: np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. start/end: int, optional Plot will we timeseries[start: end], to exclude transient. \"\"\" assert ( outputs_true . shape == outputs_pred . shape ), \"true and pred outputs must have same shape\" assert ( len ( lags ) == outputs_true . shape [ 1 ] ), \"second dimension of outputs must equal len(lags)\" n_lags = len ( lags ) # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_lags / 2 )), ncols = 2 , figsize = ( 18 , 2.0 * n_lags ) ) for lag_idx , lag in enumerate ( lags ): ax = axes . flat [ lag_idx ] plot_predicted_ts ( outputs_true [:, lag_idx ], outputs_pred [:, lag_idx ], start = start , end = end , title = f \"lag = { lag } \" , ax = ax , legend = False , ) handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"upper center\" , fontsize = 20 , ncol = 2 , fancybox = True , shadow = True , ) if n_lags % 2 != 0 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout ()","title":"plotting"},{"location":"api/plotting/#echoes.plotting._core.plot_predicted_ts","text":"Parameters: Name Type Description Default ts_true Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Target time series. required ts_pred Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Predicted time series. required start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None ax Axes plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None None title str str,optional Plot title. '' figsize Tuple tuple Figure size. Default (6, 2). (6, 2) legend bool bool If True, legend is added (\"target\", \"predicted\"). True Returns: Type Description ax matplotlib Axes Returns the Axes object with the plot drawn onto it. Source code in echoes/plotting/_core.py def plot_predicted_ts ( ts_true : Union [ np . ndarray , List , pd . Series ], ts_pred : Union [ np . ndarray , List , pd . Series ], start : int = None , end : int = None , ax : plt . Axes = None , title : str = \"\" , figsize : Tuple = ( 6 , 2 ), legend : bool = True , ) -> None : \"\"\" Arguments: ts_true: np.ndarray, List, pd.Series Target time series. ts_pred: np.ndarray, List, pd.Series Predicted time series. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. ax: plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None title: str,optional Plot title. figsize: tuple Figure size. Default (6, 2). legend: bool If True, legend is added (\"target\", \"predicted\"). Returns: ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. \"\"\" if isinstance ( ts_true , pd . Series ): ts_true = ts_true . values if isinstance ( ts_pred , pd . Series ): ts_pred = ts_pred . values if ax is None : fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) ax . plot ( ts_true [ start : end ], color = \"steelblue\" , label = \"target\" , linewidth = 5.5 ) ax . set_xlabel ( \"time\" ) ax . plot ( ts_pred [ start : end ], linestyle = \"--\" , color = \"orange\" , linewidth = 2 , label = \"prediction\" , ) ax . set_ylabel ( \"output value\" ) ax . set_xlabel ( \"time\" ) if legend : ax . legend () return ax","title":"plot_predicted_ts()"},{"location":"api/plotting/#echoes.plotting._core.plot_reservoir_activity","text":"Plot the activity, ie time series of states, of the reservoir neurons. Parameters: Name Type Description Default esn Union[echoes.esn._regressor.ESNRegressor, echoes.esn._generator.ESNGenerator] ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. required neurons Union[numpy.ndarray, List] np.ndarray or List List of reservoir neurons indices whose time series will be plotted. required train bool bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. False pred bool bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. True start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None suptitle str, optional Plot suptitle. required figsize Tuple tuple Figure size. Default (15, 10). (15, 9) kwargs dict Plotting kwargs passed to plt.plot {} Returns: Type Description fig plt.figure Figure object for fine tuning. Source code in echoes/plotting/_core.py def plot_reservoir_activity ( esn : Union [ ESNRegressor , ESNGenerator ], neurons : Union [ np . ndarray , List ], train : bool = False , pred : bool = True , start : int = None , end : int = None , figsize : Tuple = ( 15 , 9 ), ** kwargs , ): \"\"\" Plot the activity, ie time series of states, of the reservoir neurons. Arguments: esn: ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. neurons: np.ndarray or List List of reservoir neurons indices whose time series will be plotted. train: bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. pred: bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. suptitle: str, optional Plot suptitle. figsize: tuple Figure size. Default (15, 10). kwargs: dict Plotting kwargs passed to plt.plot Returns: fig: plt.figure Figure object for fine tuning. \"\"\" assert train or pred , \"either train or pred must be True\" assert not ( train and pred ), \"only one of train or pred can be True\" n_neurons = len ( neurons ) # Grab time series to plot ts = esn . states_pred_ if pred else esn . states_train_ # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_neurons / 3 )), ncols = 3 , figsize = figsize ) if \"linewidth\" in kwargs : linewidth = kwargs . pop ( \"linewidht\" ) else : linewidth = 3 if \"color\" in kwargs : color = kwargs . pop ( \"color\" ) else : color = \".6\" for neuron_idx , neuron in enumerate ( neurons ): ax = axes . flat [ neuron_idx ] ax . plot ( ts [ start : end , neuron ], linewidth = linewidth , color = color , ** kwargs ) ax . set_ylabel ( \"state\" ) ax . set_xlabel ( \"time\" ) ax . set_title ( f \"reservoir neuron idx: { neuron } \" ) # Delete unnecessary axes if n_neurons % 3 == 1 : fig . delaxes ( axes . flat [ - 1 ]) fig . delaxes ( axes . flat [ - 2 ]) elif n_neurons % 3 == 2 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout () return fig","title":"plot_reservoir_activity()"},{"location":"api/plotting/#echoes.plotting._core.set_mystyle","text":"Set context and a couple of defaults for nicer plots. Source code in echoes/plotting/_core.py def set_mystyle (): \"\"\"Set context and a couple of defaults for nicer plots.\"\"\" sns . set ( context = \"paper\" , style = \"whitegrid\" , font_scale = 1.4 , rc = { \"grid.linestyle\" : \"--\" , \"grid.linewidth\" : 0.8 }, ) Plotting functions related to the Memory Capacity task.","title":"set_mystyle()"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_forgetting_curve","text":"Plot forgetting curve, ie, memory capacity (MC) vs lag. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Sequence of lags used in the memory capacity task. required forgetting_curve ndarray np.ndarray Sequence of results from the memory task. required ax Axes plt.Axes, optional If given plot will use this axes. None kwargs mapping, optional Plotting args passed to ax.plot. {} Source code in echoes/plotting/_memory_capacity.py def plot_forgetting_curve ( lags : Union [ List , np . ndarray ], forgetting_curve : np . ndarray , ax : plt . Axes = None , ** kwargs , ) -> None : \"\"\" Plot forgetting curve, ie, memory capacity (MC) vs lag. Arguments: lags: np.ndarray or List Sequence of lags used in the memory capacity task. forgetting_curve: np.ndarray Sequence of results from the memory task. ax: plt.Axes, optional If given plot will use this axes. kwargs: mapping, optional Plotting args passed to ax.plot. \"\"\" if ax is None : fig , ax = plt . subplots () ax . plot ( lags , forgetting_curve , ** kwargs ) ax . set_xlabel ( \"$k$\" ) ax . set_ylabel ( r \"$MC_k$\" )","title":"plot_forgetting_curve()"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_mc_predicted_ts","text":"Plot true and predicted time series coming from memory capacity task for all lags. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). required ouputs_true np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. required ouputs_pred np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. required start/end int, optional Plot will we timeseries[start: end], to exclude transient. required Source code in echoes/plotting/_memory_capacity.py def plot_mc_predicted_ts ( lags : Union [ List , np . ndarray ], outputs_true : np . ndarray , outputs_pred : np . ndarray , start : int = None , end : int = None , ) -> None : \"\"\" Plot true and predicted time series coming from memory capacity task for all lags. Arguments: lags: np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). ouputs_true: np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. ouputs_pred: np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. start/end: int, optional Plot will we timeseries[start: end], to exclude transient. \"\"\" assert ( outputs_true . shape == outputs_pred . shape ), \"true and pred outputs must have same shape\" assert ( len ( lags ) == outputs_true . shape [ 1 ] ), \"second dimension of outputs must equal len(lags)\" n_lags = len ( lags ) # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_lags / 2 )), ncols = 2 , figsize = ( 18 , 2.0 * n_lags ) ) for lag_idx , lag in enumerate ( lags ): ax = axes . flat [ lag_idx ] plot_predicted_ts ( outputs_true [:, lag_idx ], outputs_pred [:, lag_idx ], start = start , end = end , title = f \"lag = { lag } \" , ax = ax , legend = False , ) handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"upper center\" , fontsize = 20 , ncol = 2 , fancybox = True , shadow = True , ) if n_lags % 2 != 0 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout ()","title":"plot_mc_predicted_ts()"},{"location":"api/utils/","text":"Auxiliar functions check_func_inverse ( func , inv_func ) \u00b6 check that func and inv_func are indeed inverse of each other Source code in echoes/utils.py def check_func_inverse ( func : Callable , inv_func : Callable ) -> None : \"\"\"check that func and inv_func are indeed inverse of each other\"\"\" x = np . linspace ( - 2 , 2 , 10 ) y = func ( x ) mismatch = np . where ( inv_func ( y ) != x )[ 0 ] assert np . isclose ( inv_func ( y ), x ) . all (), f \"function { inv_func . __name__ } is not the inverse of { func . __name__ } \" check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) \u00b6 Check shapes of W, W_in, W_fb Source code in echoes/utils.py def check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ): \"\"\"Check shapes of W, W_in, W_fb\"\"\" assert W . shape [ 0 ] == W . shape [ 1 ], \"W must be square\" assert len ( W ) == n_reservoir , \"W does not match n_reservoir\" assert W_in . shape [ 0 ] == n_reservoir , \"W_in first dimension must equal n_reservoir\" assert W_in . shape [ 1 ] == n_inputs , \"W_in second dimension must equal n_inputs\" if feedback : assert W_fb is not None , \"W_fb must be specified if feedback=True\" if W_fb is not None : assert ( W_fb . shape [ 0 ] == n_reservoir ), \"W_fb first dimension must equal n_reservoir\" assert W_fb . shape [ 1 ] == n_outputs , \"W_fb second dimension must equal n_outputs\" check_model_params ( params ) \u00b6 check consistency of parameters, shapes, sensible warnings Source code in echoes/utils.py def check_model_params ( params : Mapping ,) -> None : \"\"\"check consistency of parameters, shapes, sensible warnings\"\"\" W_in = params [ \"W_in_\" ] W = params [ \"W_\" ] W_fb = params [ \"W_fb_\" ] n_reservoir = params [ \"n_reservoir_\" ] n_inputs = params [ \"n_inputs_\" ] n_outputs = params [ \"n_outputs_\" ] feedback = params [ \"feedback\" ] input_scaling = params [ \"input_scaling\" ] input_shift = params [ \"input_shift\" ] check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) check_sparsity ( params [ \"sparsity\" ]) check_input_scaling ( input_scaling , n_inputs ) check_input_shift ( input_scaling , n_inputs ) # Warnings if params [ \"leak_rate\" ] == 0 : warnings . warn ( \"leak_rate == 0 is total leakeage, you probably meant 1. See documentation.\" ) if ( params [ \"regression_method\" ] != \"ridge\" and params [ \"ridge_sample_weight\" ] is not None ): warnings . warn ( \"ridge_sample_weight will be ignored since regression_method is not ridge\" ) relu ( x ) \u00b6 Numba jitted ReLu (rectified linear unit) function. Return ReLu(x) Source code in echoes/utils.py @njit def relu ( x : Union [ float , int , np . ndarray ]) -> Union [ float , np . ndarray ]: \"\"\"Numba jitted ReLu (rectified linear unit) function. Return ReLu(x)\"\"\" return np . maximum ( 0 , x ) set_spectral_radius ( matrix , target_radius ) \u00b6 Return a copy of matrix with rescaled weights to match target spectral radius Source code in echoes/utils.py def set_spectral_radius ( matrix : np . ndarray , target_radius : float ) -> np . ndarray : \"\"\"Return a copy of matrix with rescaled weights to match target spectral radius\"\"\" matrix = matrix . copy () current_radius = np . max ( np . abs ( np . linalg . eigvals ( matrix ))) matrix *= target_radius / current_radius return matrix sigmoid ( x , a = 1 ) \u00b6 Return f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function. Source code in echoes/utils.py @njit def sigmoid ( x : Union [ float , int , np . ndarray ], a : float = 1 ) -> Union [ float , np . ndarray ]: \"\"\" Return f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function. \"\"\" return 1 / ( 1 + np . exp ( - x * a )) tanh ( x ) \u00b6 Numba jitted tanh function. Return tanh(x) Source code in echoes/utils.py @njit def tanh ( x : Union [ float , int , np . ndarray ]) -> Union [ float , np . ndarray ]: \"\"\"Numba jitted tanh function. Return tanh(x)\"\"\" return np . tanh ( x )","title":"utils"},{"location":"api/utils/#echoes.utils.check_func_inverse","text":"check that func and inv_func are indeed inverse of each other Source code in echoes/utils.py def check_func_inverse ( func : Callable , inv_func : Callable ) -> None : \"\"\"check that func and inv_func are indeed inverse of each other\"\"\" x = np . linspace ( - 2 , 2 , 10 ) y = func ( x ) mismatch = np . where ( inv_func ( y ) != x )[ 0 ] assert np . isclose ( inv_func ( y ), x ) . all (), f \"function { inv_func . __name__ } is not the inverse of { func . __name__ } \"","title":"check_func_inverse()"},{"location":"api/utils/#echoes.utils.check_matrices_shapes","text":"Check shapes of W, W_in, W_fb Source code in echoes/utils.py def check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ): \"\"\"Check shapes of W, W_in, W_fb\"\"\" assert W . shape [ 0 ] == W . shape [ 1 ], \"W must be square\" assert len ( W ) == n_reservoir , \"W does not match n_reservoir\" assert W_in . shape [ 0 ] == n_reservoir , \"W_in first dimension must equal n_reservoir\" assert W_in . shape [ 1 ] == n_inputs , \"W_in second dimension must equal n_inputs\" if feedback : assert W_fb is not None , \"W_fb must be specified if feedback=True\" if W_fb is not None : assert ( W_fb . shape [ 0 ] == n_reservoir ), \"W_fb first dimension must equal n_reservoir\" assert W_fb . shape [ 1 ] == n_outputs , \"W_fb second dimension must equal n_outputs\"","title":"check_matrices_shapes()"},{"location":"api/utils/#echoes.utils.check_model_params","text":"check consistency of parameters, shapes, sensible warnings Source code in echoes/utils.py def check_model_params ( params : Mapping ,) -> None : \"\"\"check consistency of parameters, shapes, sensible warnings\"\"\" W_in = params [ \"W_in_\" ] W = params [ \"W_\" ] W_fb = params [ \"W_fb_\" ] n_reservoir = params [ \"n_reservoir_\" ] n_inputs = params [ \"n_inputs_\" ] n_outputs = params [ \"n_outputs_\" ] feedback = params [ \"feedback\" ] input_scaling = params [ \"input_scaling\" ] input_shift = params [ \"input_shift\" ] check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) check_sparsity ( params [ \"sparsity\" ]) check_input_scaling ( input_scaling , n_inputs ) check_input_shift ( input_scaling , n_inputs ) # Warnings if params [ \"leak_rate\" ] == 0 : warnings . warn ( \"leak_rate == 0 is total leakeage, you probably meant 1. See documentation.\" ) if ( params [ \"regression_method\" ] != \"ridge\" and params [ \"ridge_sample_weight\" ] is not None ): warnings . warn ( \"ridge_sample_weight will be ignored since regression_method is not ridge\" )","title":"check_model_params()"},{"location":"api/utils/#echoes.utils.relu","text":"Numba jitted ReLu (rectified linear unit) function. Return ReLu(x) Source code in echoes/utils.py @njit def relu ( x : Union [ float , int , np . ndarray ]) -> Union [ float , np . ndarray ]: \"\"\"Numba jitted ReLu (rectified linear unit) function. Return ReLu(x)\"\"\" return np . maximum ( 0 , x )","title":"relu()"},{"location":"api/utils/#echoes.utils.set_spectral_radius","text":"Return a copy of matrix with rescaled weights to match target spectral radius Source code in echoes/utils.py def set_spectral_radius ( matrix : np . ndarray , target_radius : float ) -> np . ndarray : \"\"\"Return a copy of matrix with rescaled weights to match target spectral radius\"\"\" matrix = matrix . copy () current_radius = np . max ( np . abs ( np . linalg . eigvals ( matrix ))) matrix *= target_radius / current_radius return matrix","title":"set_spectral_radius()"},{"location":"api/utils/#echoes.utils.sigmoid","text":"Return f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function. Source code in echoes/utils.py @njit def sigmoid ( x : Union [ float , int , np . ndarray ], a : float = 1 ) -> Union [ float , np . ndarray ]: \"\"\" Return f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function. \"\"\" return 1 / ( 1 + np . exp ( - x * a ))","title":"sigmoid()"},{"location":"api/utils/#echoes.utils.tanh","text":"Numba jitted tanh function. Return tanh(x) Source code in echoes/utils.py @njit def tanh ( x : Union [ float , int , np . ndarray ]) -> Union [ float , np . ndarray ]: \"\"\"Numba jitted tanh function. Return tanh(x)\"\"\" return np . tanh ( x )","title":"tanh()"},{"location":"examples/plot_generator_mackeyglass17/","text":"ESNGenerator example This example shows how to use Echo State Networks as pattern generator to produce a Mackey-Glass system . We try to predict the future steps of a chaotic time series. For testing, we use the Echo State Network in \"generative mode\", which means, we do not have any input (only the bias) and the output is fed back into the network for producing the next time step. Several parameter used in the example are arbitrary and even not so conventional (e.g., spectral radius > 1), just just for the sake of the example. Other constellations produce satisfactory results too, so feel free to play around with them. import numpy as np from matplotlib import pyplot as plt from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import plot_predicted_ts , set_mystyle set_mystyle () # optional, set aesthetics # Load and split data mackey_ts = load_mackeyglasst17 () n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( mackey_ts , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) esn = ESNGenerator ( n_steps = n_test_steps , n_reservoir = 200 , spectral_radius = 1.25 , leak_rate = .4 , random_state = 42 , ) # Fit the model. Inputs is None because we only have the target time series esn . fit ( X = None , y = y_train ) y_pred = esn . predict () print ( \"test r2 score\" , r2_score ( y_test , y_pred )) # Plot training and test plt . figure ( figsize = ( 22 , 5 )) plt . plot ( mackey_ts [: n_total_steps ], 'steelblue' , linewidth = 5 , label = \"target system\" ) plt . plot ( esn . training_prediction_ , color = \"y\" , linewidth = 1 , label = \"training fit\" ) plt . plot ( range ( n_train_steps , n_total_steps ), y_pred , 'orange' , label = \"ESNGenerator\" ) plt . ylabel ( \"oscillator value\" ) plt . xlabel ( 'time' ) lo , hi = plt . ylim () plt . vlines ( n_train_steps , lo - .05 , hi + .05 , linestyles = '--' ) plt . legend ( fontsize = 'small' ) # Plot test alone plt . figure ( figsize = ( 22 , 5 )) plt . subplot ( 1 , 4 , ( 1 , 3 )) plt . title ( \"zoom into test\" ) plt . plot ( y_test , color = \"steelblue\" , label = \"target system\" , linewidth = 5.5 ) plt . xlabel ( 'time' ) plt . plot ( y_pred , linestyle = '--' , color = \"orange\" , linewidth = 2 , label = \"generative ESN\" ,) plt . ylabel ( \"oscillator\" ) plt . xlabel ( 'time' ) plt . legend ( fontsize = 'small' ) plt . subplot ( 1 , 4 , 4 ) plt . title ( r \"$W^ {out} $ weights distribution\" ) plt . xlabel ( 'weight' ) plt . ylabel ( 'frequency' ) plt . hist ( esn . W_out_ . flat ) plt . tight_layout (); test r2 score 0.4041090675570501","title":"ESNGenerator (Mackey-Glass)"},{"location":"examples/plot_regressor_sincos/","text":"ESNRegressor example This example shows a minimal example of Echo State Network applied to a regression problem. import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from echoes import ESNRegressor from echoes.plotting import set_mystyle set_mystyle () # optional: set aesthetics # Prepare synthetic data x = np . linspace ( 0 , 30 * np . pi , 1000 ) . reshape ( - 1 , 1 ) inputs = np . sin ( x ) + np . random . normal ( scale = .1 , size = x . shape ) outputs = np . cos ( x ) + np . random . normal ( scale = .1 , size = x . shape ) X_train , X_test , y_train , y_test = train_test_split ( inputs , outputs , test_size = .3 , shuffle = False ) esn = ESNRegressor ( spectral_radius = .95 , leak_rate = .4 , n_transient = 100 , regression_method = \"pinv\" , random_state = 42 ) esn . fit ( X_train , y_train ) print ( \"training r2 score: \" , esn . score ( X_train , y_train )) print ( \"test r2 score: \" , esn . score ( X_test , y_test )) # Get prediction for plotting y_pred = esn . predict ( X_test ) plt . figure ( figsize = ( 15 , 4 )) plt . subplot ( 1 , 3 , ( 1 , 2 )) plt . plot ( y_test [ esn . n_transient :], label = 'target signal' , color = \"steelblue\" , linewidth = 5.5 ) plt . plot ( y_pred [ esn . n_transient :], label = 'predicted signal' , linestyle = '--' , color = \"orange\" , linewidth = 2 ,) plt . ylabel ( \"oscillator\" ) plt . xlabel ( 'time' ) plt . legend ( fontsize = ( \"small\" ), loc = 2 ) plt . subplot ( 1 , 3 , 3 ) plt . title ( r \"$W^ {out} $ weights distribution\" ) plt . xlabel ( 'weight' ) plt . ylabel ( 'frequency' ) plt . hist ( esn . W_out_ . flat ); training r2 score: 0.9840476267330117 test r2 score: 0.9733774695460478","title":"ESNRegressor (sin-cos)"},{"location":"examples/plot_reservoir_activity/","text":"Visualization of reservoir neurons activity \u00b6 This example shows how to access the time series of the reservoir neurons activity during the prediction phase. We use the ESNGenerator model as a pattern generator that learns to predict future steps of the chaotic time series Mackey-Glass Then we plot some randomly picked neurons from the reservoir. import numpy as np from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import ( set_mystyle , plot_reservoir_activity , plot_predicted_ts ) set_mystyle () # just aesthetics # Load data and define train/test length data = load_mackeyglasst17 () . reshape ( - 1 , 1 ) n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( data , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) # Instantiate model esn = ESNGenerator ( n_reservoir = 200 , n_steps = n_test_steps , spectral_radius = 1.25 , leak_rate = .4 , regression_method = \"pinv\" , store_states_pred = True , # store states to plot later random_state = 42 , ) . fit ( None , y = y_train ) prediction = esn . predict () ax = plot_predicted_ts ( y_test , prediction , end = 500 , figsize = ( 12 , 3 ) ) # We can customize the plot ax . set_title ( \"My prediction\" ) ax . legend ( loc = 1 , fancybox = True , shadow = True ) # Pick 9 random neurons to plot neurons_to_plot = sorted ( np . random . randint ( 0 , esn . n_reservoir , size = 9 )) # This plots the activity and return the fig object for finetuning fig = plot_reservoir_activity ( esn , neurons_to_plot , pred = True , # plot activity during prediction end = 500 , figsize = ( 12 , 8 ), color = \"tab:orange\" , alpha = .7 ) # Optional finetuning for i , ax in enumerate ( fig . axes ): if i % 2 == 0 : ax . set_title ( \"$ \\\\ bf{CUSTOM-TITLE}$\" )","title":"Plot Reservoir Activity"},{"location":"examples/plot_reservoir_activity/#visualization-of-reservoir-neurons-activity","text":"This example shows how to access the time series of the reservoir neurons activity during the prediction phase. We use the ESNGenerator model as a pattern generator that learns to predict future steps of the chaotic time series Mackey-Glass Then we plot some randomly picked neurons from the reservoir. import numpy as np from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import ( set_mystyle , plot_reservoir_activity , plot_predicted_ts ) set_mystyle () # just aesthetics # Load data and define train/test length data = load_mackeyglasst17 () . reshape ( - 1 , 1 ) n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( data , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) # Instantiate model esn = ESNGenerator ( n_reservoir = 200 , n_steps = n_test_steps , spectral_radius = 1.25 , leak_rate = .4 , regression_method = \"pinv\" , store_states_pred = True , # store states to plot later random_state = 42 , ) . fit ( None , y = y_train ) prediction = esn . predict () ax = plot_predicted_ts ( y_test , prediction , end = 500 , figsize = ( 12 , 3 ) ) # We can customize the plot ax . set_title ( \"My prediction\" ) ax . legend ( loc = 1 , fancybox = True , shadow = True ) # Pick 9 random neurons to plot neurons_to_plot = sorted ( np . random . randint ( 0 , esn . n_reservoir , size = 9 )) # This plots the activity and return the fig object for finetuning fig = plot_reservoir_activity ( esn , neurons_to_plot , pred = True , # plot activity during prediction end = 500 , figsize = ( 12 , 8 ), color = \"tab:orange\" , alpha = .7 ) # Optional finetuning for i , ax in enumerate ( fig . axes ): if i % 2 == 0 : ax . set_title ( \"$ \\\\ bf{CUSTOM-TITLE}$\" )","title":"Visualization of reservoir neurons activity"},{"location":"tutorial/","text":"Coming soon! Meanwhile you can take a look at this article by H. Jaeger.","title":"What are Echo State Networks?"}]}