{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"echoes \u00b6 (work in progress!) Scikit-learn compatible, high level API for machine learning with Echo State Networks (ESN). Check out the examples for a quick start. Installation \u00b6 Download or clone repo like this: git clone https://github.com/fabridamicelli/echoes Then cd to directory and pip install: cd echoes pip install . Recommendation: install the package in a separate virtual environment, e.g., created with (mini)conda . Citing \u00b6 If you find echoes useful for a publication, then please use the following BibTeX to cite it: @misc{echoes, author = {Damicelli, Fabrizio}, title = {echoes: Echo State Networks with Python}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\\url{https://github.com/fabridamicelli/echoes}}, } Requirements \u00b6 Dependencies \u00b6 numpy sklearn pandas matplotlib seaborn tests: mypy, pytest The code has been tested with Python 3.7 on Ubuntu 16.04. Datasets \u00b6 Mackey-Glass-t17 Tests \u00b6 Run tests with make test","title":"Home"},{"location":"#echoes","text":"(work in progress!) Scikit-learn compatible, high level API for machine learning with Echo State Networks (ESN). Check out the examples for a quick start.","title":"echoes"},{"location":"#installation","text":"Download or clone repo like this: git clone https://github.com/fabridamicelli/echoes Then cd to directory and pip install: cd echoes pip install . Recommendation: install the package in a separate virtual environment, e.g., created with (mini)conda .","title":"Installation"},{"location":"#citing","text":"If you find echoes useful for a publication, then please use the following BibTeX to cite it: @misc{echoes, author = {Damicelli, Fabrizio}, title = {echoes: Echo State Networks with Python}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\\url{https://github.com/fabridamicelli/echoes}}, }","title":"Citing"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#dependencies","text":"numpy sklearn pandas matplotlib seaborn tests: mypy, pytest The code has been tested with Python 3.7 on Ubuntu 16.04.","title":"Dependencies"},{"location":"#datasets","text":"Mackey-Glass-t17","title":"Datasets"},{"location":"#tests","text":"Run tests with make test","title":"Tests"},{"location":"api/ESNGenerator/","text":"\u00b6 The number of inputs (n_inputs) is always 1 and n_outputs is infered from passed data. It uses always feedback, so that is not a parameter anymore (always True). Parameters: Name Type Description Default n_steps int, default=100 Number of steps to generate pattern (used by predict method). required n_reservoir int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed. required W np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed. required spectral_radius float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W). required W_in np.ndarray of shape (n_reservoir, 1+n_inputs) (1->bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used. required W_fb np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback. required sparsity float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored. required noise float, optional, default=0 Magnitud of the noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5. required leak_rate float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage. required bias float, optional, default=1 Value of the bias neuron, injected at each time step together with input. required activation function, optional, default=tanh Non-linear activation function applied to the neurons at each step. required activation_out function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed. required inv_activation_out function, optional, default=identity Inverse of acivation function applied to the outputs. This is used to first transform targets (during training). required fit_only_states bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase. required regression_method str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used. required ridge_alpha float, ndarray of shape (n_outputs,), default=None Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details. required ridge_fit_intercept bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details. required ridge_normalize bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. See sklearn Ridge documentation for details. required ridge_max_iter int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details. required ridge_tol float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details. required ridge_solver str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details. required ridge_sample_weight float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details. required n_transient int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak. required random_state int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . required store_states_train bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_. required store_states_pred bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_. required Attributes: \u00b6 - W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1). Outgoing weights after fitting linear regression model to predict outputs. - training_prediction_: array of shape (n_samples, n_outputs). Predicted output on training data. - states_train_: array of shape (n_samples, n_reservoir), default False. If store_states_train is True, states matrix is stored for visualizing reservoir neurons activity during training. - states_pred_: array of shape (n_samples, n_reservoir), default False. If store_states_pred is True, states matrix is stored for visualizing reservoir neurons activity during prediction (test). fit ( self , X = None , y = None ) \u00b6 Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. None Returns: Type Description ESNGenerator self: returns an instance of self. Source code in echoes/esn/_generator.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def fit ( self , X = None , y = None ) -> \"ESNGenerator\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. Returns: self: returns an instance of self. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator only takes y for training\" ) y = check_array ( y , ensure_2d = False ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) outputs = y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) # Pattern generation takes no input, thus hardcode for later # construction of matrices self . n_inputs_ = 1 self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) # Make inputs zero inputs = np . zeros ( shape = ( outputs . shape [ 0 ], self . n_inputs_ )) check_consistent_length ( inputs , outputs ) # sanity check # Inverse transform outputs (map them into inner, latent space) outputs = self . inv_activation_out ( outputs ) n_samples = inputs . shape [ 0 ] # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Collect reservoir states through the given input,output pairs states = np . zeros (( n_samples , self . n_reservoir_ )) for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 ], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) # Extend states matrix with inputs (and bias); i.e., make [x(t); 1; u(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (map them back to original space with activation) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Keep last state for later self . last_state = states [ - 1 , :] self . last_input = inputs [ - 1 , :] self . last_output = outputs [ - 1 , :] # Store reservoir activity if self . store_states_train : self . states_train_ = states return self predict ( self , X = None ) \u00b6 Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Parameters: Name Type Description Default X None, always ignored, API consistency None Returns: Type Description ndarray outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. Source code in echoes/esn/_generator.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def predict ( self , X = None ) -> np . ndarray : \"\"\" Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Arguments: X: None, always ignored, API consistency Returns: outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator takes no X for prediction\" ) n_steps = self . n_steps assert n_steps >= 1 , \"n_steps must be >= 1\" # Scale and shift inputs are ignored, since only bias should contribute to # next state and inputs should be zero inputs = np . zeros ( shape = ( n_steps , self . n_inputs_ )) # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_steps , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Initialize predictions: begin with last state as first state inputs = np . vstack ([ self . last_input , inputs ]) states = np . vstack ([ self . last_state , np . zeros (( n_steps , self . n_reservoir_ ))]) outputs = np . vstack ([ self . last_output , np . zeros (( n_steps , self . n_outputs_ ))]) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for step in range ( 1 , outputs . shape [ 0 ]): states [ step , :] = self . _update_state ( states [ step - 1 , :], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) if self . fit_only_states : full_states = states [ step , :] else : full_states = np . concatenate ([ states [ step , :], inputs [ step , :]]) # Predict outputs [ step , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states [ 1 :, :] # discard first step (comes from fitting) # Map outputs back to actual target space with activation function outputs = self . activation_out ( outputs ) return outputs [ 1 :, :] # discard initial step (comes from fitting) score ( self , X = None , y = None , sample_weight = None ) \u00b6 Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. None y 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. None Returns: Type Description float score: float R2 score Source code in echoes/esn/_generator.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. Returns: score: float R2 score \"\"\" return r2_score ( y , self . predict (), sample_weight = sample_weight )","title":"ESNGenerator"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator","text":"The number of inputs (n_inputs) is always 1 and n_outputs is infered from passed data. It uses always feedback, so that is not a parameter anymore (always True). Parameters: Name Type Description Default n_steps int, default=100 Number of steps to generate pattern (used by predict method). required n_reservoir int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed. required W np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed. required spectral_radius float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W). required W_in np.ndarray of shape (n_reservoir, 1+n_inputs) (1->bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used. required W_fb np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback. required sparsity float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored. required noise float, optional, default=0 Magnitud of the noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5. required leak_rate float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage. required bias float, optional, default=1 Value of the bias neuron, injected at each time step together with input. required activation function, optional, default=tanh Non-linear activation function applied to the neurons at each step. required activation_out function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed. required inv_activation_out function, optional, default=identity Inverse of acivation function applied to the outputs. This is used to first transform targets (during training). required fit_only_states bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase. required regression_method str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used. required ridge_alpha float, ndarray of shape (n_outputs,), default=None Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details. required ridge_fit_intercept bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details. required ridge_normalize bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. See sklearn Ridge documentation for details. required ridge_max_iter int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details. required ridge_tol float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details. required ridge_solver str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details. required ridge_sample_weight float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details. required n_transient int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak. required random_state int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn: If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . required store_states_train bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_. required store_states_pred bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_. required","title":"echoes.esn._generator.ESNGenerator"},{"location":"api/ESNGenerator/#attributes","text":"- W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1). Outgoing weights after fitting linear regression model to predict outputs. - training_prediction_: array of shape (n_samples, n_outputs). Predicted output on training data. - states_train_: array of shape (n_samples, n_reservoir), default False. If store_states_train is True, states matrix is stored for visualizing reservoir neurons activity during training. - states_pred_: array of shape (n_samples, n_reservoir), default False. If store_states_pred is True, states matrix is stored for visualizing reservoir neurons activity during prediction (test).","title":"Attributes:"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.fit","text":"Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. None Returns: Type Description ESNGenerator self: returns an instance of self. Source code in echoes/esn/_generator.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def fit ( self , X = None , y = None ) -> \"ESNGenerator\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None, always ignored, API consistency It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None Target variable. Returns: self: returns an instance of self. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator only takes y for training\" ) y = check_array ( y , ensure_2d = False ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) outputs = y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) # Pattern generation takes no input, thus hardcode for later # construction of matrices self . n_inputs_ = 1 self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) # Make inputs zero inputs = np . zeros ( shape = ( outputs . shape [ 0 ], self . n_inputs_ )) check_consistent_length ( inputs , outputs ) # sanity check # Inverse transform outputs (map them into inner, latent space) outputs = self . inv_activation_out ( outputs ) n_samples = inputs . shape [ 0 ] # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Collect reservoir states through the given input,output pairs states = np . zeros (( n_samples , self . n_reservoir_ )) for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 ], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) # Extend states matrix with inputs (and bias); i.e., make [x(t); 1; u(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (map them back to original space with activation) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Keep last state for later self . last_state = states [ - 1 , :] self . last_input = inputs [ - 1 , :] self . last_output = outputs [ - 1 , :] # Store reservoir activity if self . store_states_train : self . states_train_ = states return self","title":"fit()"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.predict","text":"Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Parameters: Name Type Description Default X None, always ignored, API consistency None Returns: Type Description ndarray outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. Source code in echoes/esn/_generator.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def predict ( self , X = None ) -> np . ndarray : \"\"\" Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction. Arguments: X: None, always ignored, API consistency Returns: outputs: 2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs. \"\"\" if X is not None : warnings . warn ( \"X will be ignored \u2013 ESNGenerator takes no X for prediction\" ) n_steps = self . n_steps assert n_steps >= 1 , \"n_steps must be >= 1\" # Scale and shift inputs are ignored, since only bias should contribute to # next state and inputs should be zero inputs = np . zeros ( shape = ( n_steps , self . n_inputs_ )) # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_steps , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Initialize predictions: begin with last state as first state inputs = np . vstack ([ self . last_input , inputs ]) states = np . vstack ([ self . last_state , np . zeros (( n_steps , self . n_reservoir_ ))]) outputs = np . vstack ([ self . last_output , np . zeros (( n_steps , self . n_outputs_ ))]) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for step in range ( 1 , outputs . shape [ 0 ]): states [ step , :] = self . _update_state ( states [ step - 1 , :], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) if self . fit_only_states : full_states = states [ step , :] else : full_states = np . concatenate ([ states [ step , :], inputs [ step , :]]) # Predict outputs [ step , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states [ 1 :, :] # discard first step (comes from fitting) # Map outputs back to actual target space with activation function outputs = self . activation_out ( outputs ) return outputs [ 1 :, :] # discard initial step (comes from fitting)","title":"predict()"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.score","text":"Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. None y 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. None Returns: Type Description float score: float R2 score Source code in echoes/esn/_generator.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" Wrapper around sklearn r2_score with kwargs. From sklearn: R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs. y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. Returns: score: float R2 score \"\"\" return r2_score ( y , self . predict (), sample_weight = sample_weight )","title":"score()"},{"location":"api/ESNRegressor/","text":"\u00b6 fit ( self , X , y ) \u00b6 Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X ndarray None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. required y ndarray 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. required Returns self: returns an instance of self. Source code in echoes/esn/_regressor.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def fit ( self , X : np . ndarray , y : np . ndarray ) -> \"ESNRegressor\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. Returns self: returns an instance of self. \"\"\" X , y = check_X_y ( X , y , multi_output = True ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) inputs , outputs = X , y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) self . n_inputs_ = inputs . shape [ 1 ] self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) #######--##### # Scale and shift inputs inputs = self . _scale_shift_inputs ( inputs ) # Inverse transform outputs (map them into inner, latent space) outputs = self . inv_activation_out ( outputs ) n_samples = inputs . shape [ 0 ] # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Collect reservoir states through the given input,output pairs states = np . zeros (( n_samples , self . n_reservoir_ )) for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 ], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) # Extend states matrix with inputs (and bias); i.e., make [x(t); 1; u(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (map them back to original space with activation) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Store reservoir activity if self . store_states_train : self . states_train_ = states return self predict ( self , X ) \u00b6 Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Parameters: Name Type Description Default X ndarray 2D np.ndarray of shape (n_samples, n_inputs) Testing input, i.e., X, the features. required Returns: Type Description ndarray outputs: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. Source code in echoes/esn/_regressor.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def predict ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Testing input, i.e., X, the features. Returns: outputs: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. \"\"\" check_is_fitted ( self ) X = check_array ( X ) inputs = X n_samples = inputs . shape [ 0 ] # Scale and shift inputs inputs = self . _scale_shift_inputs ( inputs ) # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Initialize predictions states = np . zeros (( n_samples , self . n_reservoir_ )) outputs = np . zeros (( n_samples , self . n_outputs_ )) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 , :], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) if self . fit_only_states : full_states = states [ step , :] else : full_states = np . concatenate ([ states [ step , :], inputs [ step , :]]) # Predict outputs [ step , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states # Map outputs back to actual target space with activation function outputs = self . activation_out ( outputs ) return outputs score ( self , X = None , y = None , sample_weight = None ) \u00b6 R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X 2D np.ndarray of shape (n_samples, n_inputs) Test samples. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Example: n_steps_to_remove = 10 weights = np.ones(outputs.shape[0]) weights[: n_steps_to_remove] = 0 score(inputs, outputs, sample_weight=weights) None Returns: Type Description float score: float R2 score Source code in echoes/esn/_regressor.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Test samples. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Example: >> n_steps_to_remove = 10 >> weights = np.ones(outputs.shape[0]) >> weights[: n_steps_to_remove] = 0 >> score(inputs, outputs, sample_weight=weights) Returns: score: float R2 score \"\"\" y_pred = self . predict ( X ) if sample_weight is None : weights = np . ones ( y . shape [ 0 ]) weights [: self . n_transient ] = 0 return r2_score ( y , y_pred , sample_weight = weights ) return r2_score ( y , y_pred , sample_weight = sample_weight )","title":"ESNRegressor"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor","text":"","title":"echoes.esn._regressor.ESNRegressor"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.fit","text":"Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Parameters: Name Type Description Default X ndarray None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. required y ndarray 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. required Returns self: returns an instance of self. Source code in echoes/esn/_regressor.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def fit ( self , X : np . ndarray , y : np . ndarray ) -> \"ESNRegressor\" : \"\"\" Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs. Arguments: X: None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target. Returns self: returns an instance of self. \"\"\" X , y = check_X_y ( X , y , multi_output = True ) if y . ndim == 1 : y = y . reshape ( - 1 , 1 ) inputs , outputs = X , y # Initialize matrices and random state self . random_state_ = check_random_state ( self . random_state ) self . n_inputs_ = inputs . shape [ 1 ] self . n_reservoir_ = len ( self . W ) if self . W is not None else self . n_reservoir self . n_outputs_ = outputs . shape [ 1 ] self . W_in_ = self . _init_incoming_weights () self . W_ = self . _init_reservoir_weights () self . W_fb_ = self . _init_feedback_weights () check_model_params ( self . __dict__ ) #######--##### # Scale and shift inputs inputs = self . _scale_shift_inputs ( inputs ) # Inverse transform outputs (map them into inner, latent space) outputs = self . inv_activation_out ( outputs ) n_samples = inputs . shape [ 0 ] # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Collect reservoir states through the given input,output pairs states = np . zeros (( n_samples , self . n_reservoir_ )) for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 ], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) # Extend states matrix with inputs (and bias); i.e., make [x(t); 1; u(t)] full_states = states if self . fit_only_states else np . hstack (( states , inputs )) # Solve for W_out using full states and outputs, excluding transient self . W_out_ = self . _solve_W_out ( full_states [ self . n_transient :, :], outputs [ self . n_transient :, :] ) # Predict on training set (map them back to original space with activation) self . training_prediction_ = self . activation_out ( full_states @ self . W_out_ . T ) # Store reservoir activity if self . store_states_train : self . states_train_ = states return self","title":"fit()"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.predict","text":"Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Parameters: Name Type Description Default X ndarray 2D np.ndarray of shape (n_samples, n_inputs) Testing input, i.e., X, the features. required Returns: Type Description ndarray outputs: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. Source code in echoes/esn/_regressor.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def predict ( self , X : np . ndarray ) -> np . ndarray : \"\"\" Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training). Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Testing input, i.e., X, the features. Returns: outputs: 2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs. \"\"\" check_is_fitted ( self ) X = check_array ( X ) inputs = X n_samples = inputs . shape [ 0 ] # Scale and shift inputs inputs = self . _scale_shift_inputs ( inputs ) # Append the bias to inputs -> [1; u(t)] bias = np . ones (( n_samples , 1 )) * self . bias inputs = np . hstack (( bias , inputs )) # Initialize predictions states = np . zeros (( n_samples , self . n_reservoir_ )) outputs = np . zeros (( n_samples , self . n_outputs_ )) check_consistent_length ( inputs , outputs ) # sanity check # Go through samples (steps) and predict for each of them for step in range ( 1 , n_samples ): states [ step , :] = self . _update_state ( states [ step - 1 , :], inputs [ step , :], outputs [ step - 1 , :], self . W_in_ , self . W_ , self . W_fb_ , ) if self . fit_only_states : full_states = states [ step , :] else : full_states = np . concatenate ([ states [ step , :], inputs [ step , :]]) # Predict outputs [ step , :] = self . W_out_ @ full_states # Store reservoir activity if self . store_states_pred : self . states_pred_ = states # Map outputs back to actual target space with activation function outputs = self . activation_out ( outputs ) return outputs","title":"predict()"},{"location":"api/ESNRegressor/#echoes.esn._regressor.ESNRegressor.score","text":"R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Parameters: Name Type Description Default X 2D np.ndarray of shape (n_samples, n_inputs) Test samples. None y 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. None sample_weight array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Example: n_steps_to_remove = 10 weights = np.ones(outputs.shape[0]) weights[: n_steps_to_remove] = 0 score(inputs, outputs, sample_weight=weights) None Returns: Type Description float score: float R2 score Source code in echoes/esn/_regressor.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def score ( self , X = None , y = None , sample_weight = None ) -> float : \"\"\" R^2 (coefficient of determination) regression score function. By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below). From sklearn: Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Arguments: X: 2D np.ndarray of shape (n_samples, n_inputs) Test samples. y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs. sample_weight: array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Example: >> n_steps_to_remove = 10 >> weights = np.ones(outputs.shape[0]) >> weights[: n_steps_to_remove] = 0 >> score(inputs, outputs, sample_weight=weights) Returns: score: float R2 score \"\"\" y_pred = self . predict ( X ) if sample_weight is None : weights = np . ones ( y . shape [ 0 ]) weights [: self . n_transient ] = 0 return r2_score ( y , y_pred , sample_weight = weights ) return r2_score ( y , y_pred , sample_weight = sample_weight )","title":"score()"},{"location":"api/plotting/","text":"\u00b6 Plotting functions often needed. Not extremely well polished, rather a tool for quick visualization. plot_predicted_ts ( ts_true , ts_pred , start = None , end = None , ax = None , title = '' , figsize = ( 6 , 2 ), legend = True ) \u00b6 Parameters: Name Type Description Default ts_true Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Target time series. required ts_pred Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Predicted time series. required start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None ax Axes plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None None title str str,optional Plot title. '' figsize Tuple tuple Figure size. Default (6, 2). (6, 2) legend bool bool If True, legend is added (\"target\", \"predicted\"). True Returns: Type Description None ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. Source code in echoes/plotting/_core.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def plot_predicted_ts ( ts_true : Union [ np . ndarray , List , pd . Series ], ts_pred : Union [ np . ndarray , List , pd . Series ], start : int = None , end : int = None , ax : plt . Axes = None , title : str = \"\" , figsize : Tuple = ( 6 , 2 ), legend : bool = True , ) -> None : \"\"\" Arguments: ts_true: np.ndarray, List, pd.Series Target time series. ts_pred: np.ndarray, List, pd.Series Predicted time series. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. ax: plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None title: str,optional Plot title. figsize: tuple Figure size. Default (6, 2). legend: bool If True, legend is added (\"target\", \"predicted\"). Returns: ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. \"\"\" if isinstance ( ts_true , pd . Series ): ts_true = ts_true . values if isinstance ( ts_pred , pd . Series ): ts_pred = ts_pred . values if ax is None : fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) ax . plot ( ts_true [ start : end ], color = \"steelblue\" , label = \"target\" , linewidth = 5.5 ) ax . set_xlabel ( \"time\" ) ax . plot ( ts_pred [ start : end ], linestyle = \"--\" , color = \"orange\" , linewidth = 2 , label = \"prediction\" , ) ax . set_ylabel ( \"output value\" ) ax . set_xlabel ( \"time\" ) if legend : ax . legend () return ax plot_reservoir_activity ( esn , neurons , train = False , pred = True , start = None , end = None , figsize = ( 15 , 9 ), ** kwargs ) \u00b6 Plot the activity, ie time series of states, of the reservoir neurons. Parameters: Name Type Description Default esn Union[echoes.esn._regressor.ESNRegressor, echoes.esn._generator.ESNGenerator] ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. required neurons Union[numpy.ndarray, List] np.ndarray or List List of reservoir neurons indices whose time series will be plotted. required train bool bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. False pred bool bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. True start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None suptitle str, optional Plot suptitle. required figsize Tuple tuple Figure size. Default (15, 10). (15, 9) kwargs Mapping dict Plotting kwargs passed to plt.plot {} Returns: Type Description fig plt.figure Figure object for fine tuning. Source code in echoes/plotting/_core.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def plot_reservoir_activity ( esn : Union [ ESNRegressor , ESNGenerator ], neurons : Union [ np . ndarray , List ], train : bool = False , pred : bool = True , start : int = None , end : int = None , figsize : Tuple = ( 15 , 9 ), ** kwargs : Mapping , ): \"\"\" Plot the activity, ie time series of states, of the reservoir neurons. Arguments: esn: ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. neurons: np.ndarray or List List of reservoir neurons indices whose time series will be plotted. train: bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. pred: bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. suptitle: str, optional Plot suptitle. figsize: tuple Figure size. Default (15, 10). kwargs: dict Plotting kwargs passed to plt.plot Returns: fig: plt.figure Figure object for fine tuning. \"\"\" assert train or pred , \"either train or pred must be True\" assert not ( train and pred ), \"only one of train or pred can be True\" n_neurons = len ( neurons ) # Grab time series to plot ts = esn . states_pred_ if pred else esn . states_train_ # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_neurons / 3 )), ncols = 3 , figsize = figsize ) if \"linewidth\" in kwargs : linewidth = kwargs . pop ( \"linewidht\" ) else : linewidth = 3 if \"color\" in kwargs : color = kwargs . pop ( \"color\" ) else : color = \".6\" for neuron_idx , neuron in enumerate ( neurons ): ax = axes . flat [ neuron_idx ] ax . plot ( ts [ start : end , neuron ], linewidth = linewidth , color = color , ** kwargs ) ax . set_ylabel ( \"state\" ) ax . set_xlabel ( \"time\" ) ax . set_title ( f \"reservoir neuron idx: { neuron } \" ) # Delete unnecessary axes if n_neurons % 3 == 1 : fig . delaxes ( axes . flat [ - 1 ]) fig . delaxes ( axes . flat [ - 2 ]) elif n_neurons % 3 == 2 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout () return fig set_mystyle () \u00b6 Set context and a couple of defaults for nicer plots. Source code in echoes/plotting/_core.py 15 16 17 18 19 20 21 22 def set_mystyle (): \"\"\"Set context and a couple of defaults for nicer plots.\"\"\" sns . set ( context = \"paper\" , style = \"whitegrid\" , font_scale = 1.4 , rc = { \"grid.linestyle\" : \"--\" , \"grid.linewidth\" : 0.8 }, ) \u00b6 Plotting functions related to the Memory Capacity task. plot_forgetting_curve ( lags , forgetting_curve , ax = None , ** kwargs ) \u00b6 Plot forgetting curve, ie, memory capacity (MC) vs lag. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Sequence of lags used in the memory capacity task. required forgetting_curve ndarray np.ndarray Sequence of results from the memory task. required ax Axes plt.Axes, optional If given plot will use this axes. None kwargs Any mapping, optional Plotting args passed to ax.plot. {} Source code in echoes/plotting/_memory_capacity.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def plot_forgetting_curve ( lags : Union [ List , np . ndarray ], forgetting_curve : np . ndarray , ax : plt . Axes = None , ** kwargs : Any , ) -> None : \"\"\" Plot forgetting curve, ie, memory capacity (MC) vs lag. Arguments: lags: np.ndarray or List Sequence of lags used in the memory capacity task. forgetting_curve: np.ndarray Sequence of results from the memory task. ax: plt.Axes, optional If given plot will use this axes. kwargs: mapping, optional Plotting args passed to ax.plot. \"\"\" if ax is None : fig , ax = plt . subplots () ax . plot ( lags , forgetting_curve , ** kwargs ) ax . set_xlabel ( \"$k$\" ) ax . set_ylabel ( r \"$MC_k$\" ) plot_mc_predicted_ts ( lags , outputs_true , outputs_pred , start = None , end = None ) \u00b6 Plot true and predicted time series coming from memory capacity task for all lags. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). required ouputs_true np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. required ouputs_pred np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. required start/end int, optional Plot will we timeseries[start: end], to exclude transient. required Source code in echoes/plotting/_memory_capacity.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def plot_mc_predicted_ts ( lags : Union [ List , np . ndarray ], outputs_true : np . ndarray , outputs_pred : np . ndarray , start : int = None , end : int = None , ) -> None : \"\"\" Plot true and predicted time series coming from memory capacity task for all lags. Arguments: lags: np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). ouputs_true: np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. ouputs_pred: np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. start/end: int, optional Plot will we timeseries[start: end], to exclude transient. \"\"\" assert ( outputs_true . shape == outputs_pred . shape ), \"true and pred outputs must have same shape\" assert ( len ( lags ) == outputs_true . shape [ 1 ] ), \"second dimension of outputs must equal len(lags)\" n_lags = len ( lags ) # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_lags / 2 )), ncols = 2 , figsize = ( 18 , 2.0 * n_lags ) ) for lag_idx , lag in enumerate ( lags ): ax = axes . flat [ lag_idx ] plot_predicted_ts ( outputs_true [:, lag_idx ], outputs_pred [:, lag_idx ], start = start , end = end , title = f \"lag = { lag } \" , ax = ax , legend = False , ) handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"upper center\" , fontsize = 20 , ncol = 2 , fancybox = True , shadow = True , ) if n_lags % 2 != 0 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout ()","title":"plotting"},{"location":"api/plotting/#echoes.plotting._core","text":"Plotting functions often needed. Not extremely well polished, rather a tool for quick visualization.","title":"echoes.plotting._core"},{"location":"api/plotting/#echoes.plotting._core.plot_predicted_ts","text":"Parameters: Name Type Description Default ts_true Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Target time series. required ts_pred Union[numpy.ndarray, List, pandas.core.series.Series] np.ndarray, List, pd.Series Predicted time series. required start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None ax Axes plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None None title str str,optional Plot title. '' figsize Tuple tuple Figure size. Default (6, 2). (6, 2) legend bool bool If True, legend is added (\"target\", \"predicted\"). True Returns: Type Description None ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. Source code in echoes/plotting/_core.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def plot_predicted_ts ( ts_true : Union [ np . ndarray , List , pd . Series ], ts_pred : Union [ np . ndarray , List , pd . Series ], start : int = None , end : int = None , ax : plt . Axes = None , title : str = \"\" , figsize : Tuple = ( 6 , 2 ), legend : bool = True , ) -> None : \"\"\" Arguments: ts_true: np.ndarray, List, pd.Series Target time series. ts_pred: np.ndarray, List, pd.Series Predicted time series. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. ax: plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None title: str,optional Plot title. figsize: tuple Figure size. Default (6, 2). legend: bool If True, legend is added (\"target\", \"predicted\"). Returns: ax: matplotlib Axes Returns the Axes object with the plot drawn onto it. \"\"\" if isinstance ( ts_true , pd . Series ): ts_true = ts_true . values if isinstance ( ts_pred , pd . Series ): ts_pred = ts_pred . values if ax is None : fig , ax = plt . subplots ( figsize = figsize ) ax . set_title ( title ) ax . plot ( ts_true [ start : end ], color = \"steelblue\" , label = \"target\" , linewidth = 5.5 ) ax . set_xlabel ( \"time\" ) ax . plot ( ts_pred [ start : end ], linestyle = \"--\" , color = \"orange\" , linewidth = 2 , label = \"prediction\" , ) ax . set_ylabel ( \"output value\" ) ax . set_xlabel ( \"time\" ) if legend : ax . legend () return ax","title":"plot_predicted_ts()"},{"location":"api/plotting/#echoes.plotting._core.plot_reservoir_activity","text":"Plot the activity, ie time series of states, of the reservoir neurons. Parameters: Name Type Description Default esn Union[echoes.esn._regressor.ESNRegressor, echoes.esn._generator.ESNGenerator] ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. required neurons Union[numpy.ndarray, List] np.ndarray or List List of reservoir neurons indices whose time series will be plotted. required train bool bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. False pred bool bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. True start int int, optional Plot will be timeseries[start: end]. None end int int, optional Plot will be timeseries[start: end]. None suptitle str, optional Plot suptitle. required figsize Tuple tuple Figure size. Default (15, 10). (15, 9) kwargs Mapping dict Plotting kwargs passed to plt.plot {} Returns: Type Description fig plt.figure Figure object for fine tuning. Source code in echoes/plotting/_core.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def plot_reservoir_activity ( esn : Union [ ESNRegressor , ESNGenerator ], neurons : Union [ np . ndarray , List ], train : bool = False , pred : bool = True , start : int = None , end : int = None , figsize : Tuple = ( 15 , 9 ), ** kwargs : Mapping , ): \"\"\" Plot the activity, ie time series of states, of the reservoir neurons. Arguments: esn: ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction. neurons: np.ndarray or List List of reservoir neurons indices whose time series will be plotted. train: bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two. pred: bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two. start: int, optional Plot will be timeseries[start: end]. end: int, optional Plot will be timeseries[start: end]. suptitle: str, optional Plot suptitle. figsize: tuple Figure size. Default (15, 10). kwargs: dict Plotting kwargs passed to plt.plot Returns: fig: plt.figure Figure object for fine tuning. \"\"\" assert train or pred , \"either train or pred must be True\" assert not ( train and pred ), \"only one of train or pred can be True\" n_neurons = len ( neurons ) # Grab time series to plot ts = esn . states_pred_ if pred else esn . states_train_ # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_neurons / 3 )), ncols = 3 , figsize = figsize ) if \"linewidth\" in kwargs : linewidth = kwargs . pop ( \"linewidht\" ) else : linewidth = 3 if \"color\" in kwargs : color = kwargs . pop ( \"color\" ) else : color = \".6\" for neuron_idx , neuron in enumerate ( neurons ): ax = axes . flat [ neuron_idx ] ax . plot ( ts [ start : end , neuron ], linewidth = linewidth , color = color , ** kwargs ) ax . set_ylabel ( \"state\" ) ax . set_xlabel ( \"time\" ) ax . set_title ( f \"reservoir neuron idx: { neuron } \" ) # Delete unnecessary axes if n_neurons % 3 == 1 : fig . delaxes ( axes . flat [ - 1 ]) fig . delaxes ( axes . flat [ - 2 ]) elif n_neurons % 3 == 2 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout () return fig","title":"plot_reservoir_activity()"},{"location":"api/plotting/#echoes.plotting._core.set_mystyle","text":"Set context and a couple of defaults for nicer plots. Source code in echoes/plotting/_core.py 15 16 17 18 19 20 21 22 def set_mystyle (): \"\"\"Set context and a couple of defaults for nicer plots.\"\"\" sns . set ( context = \"paper\" , style = \"whitegrid\" , font_scale = 1.4 , rc = { \"grid.linestyle\" : \"--\" , \"grid.linewidth\" : 0.8 }, )","title":"set_mystyle()"},{"location":"api/plotting/#echoes.plotting._memory_capacity","text":"Plotting functions related to the Memory Capacity task.","title":"echoes.plotting._memory_capacity"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_forgetting_curve","text":"Plot forgetting curve, ie, memory capacity (MC) vs lag. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Sequence of lags used in the memory capacity task. required forgetting_curve ndarray np.ndarray Sequence of results from the memory task. required ax Axes plt.Axes, optional If given plot will use this axes. None kwargs Any mapping, optional Plotting args passed to ax.plot. {} Source code in echoes/plotting/_memory_capacity.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def plot_forgetting_curve ( lags : Union [ List , np . ndarray ], forgetting_curve : np . ndarray , ax : plt . Axes = None , ** kwargs : Any , ) -> None : \"\"\" Plot forgetting curve, ie, memory capacity (MC) vs lag. Arguments: lags: np.ndarray or List Sequence of lags used in the memory capacity task. forgetting_curve: np.ndarray Sequence of results from the memory task. ax: plt.Axes, optional If given plot will use this axes. kwargs: mapping, optional Plotting args passed to ax.plot. \"\"\" if ax is None : fig , ax = plt . subplots () ax . plot ( lags , forgetting_curve , ** kwargs ) ax . set_xlabel ( \"$k$\" ) ax . set_ylabel ( r \"$MC_k$\" )","title":"plot_forgetting_curve()"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_mc_predicted_ts","text":"Plot true and predicted time series coming from memory capacity task for all lags. Parameters: Name Type Description Default lags Union[List, numpy.ndarray] np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). required ouputs_true np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. required ouputs_pred np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. required start/end int, optional Plot will we timeseries[start: end], to exclude transient. required Source code in echoes/plotting/_memory_capacity.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def plot_mc_predicted_ts ( lags : Union [ List , np . ndarray ], outputs_true : np . ndarray , outputs_pred : np . ndarray , start : int = None , end : int = None , ) -> None : \"\"\" Plot true and predicted time series coming from memory capacity task for all lags. Arguments: lags: np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5). ouputs_true: np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model. ouputs_pred: np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model. start/end: int, optional Plot will we timeseries[start: end], to exclude transient. \"\"\" assert ( outputs_true . shape == outputs_pred . shape ), \"true and pred outputs must have same shape\" assert ( len ( lags ) == outputs_true . shape [ 1 ] ), \"second dimension of outputs must equal len(lags)\" n_lags = len ( lags ) # Plot test fig , axes = plt . subplots ( nrows = int ( np . ceil ( n_lags / 2 )), ncols = 2 , figsize = ( 18 , 2.0 * n_lags ) ) for lag_idx , lag in enumerate ( lags ): ax = axes . flat [ lag_idx ] plot_predicted_ts ( outputs_true [:, lag_idx ], outputs_pred [:, lag_idx ], start = start , end = end , title = f \"lag = { lag } \" , ax = ax , legend = False , ) handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"upper center\" , fontsize = 20 , ncol = 2 , fancybox = True , shadow = True , ) if n_lags % 2 != 0 : fig . delaxes ( axes . flat [ - 1 ]) fig . tight_layout ()","title":"plot_mc_predicted_ts()"},{"location":"api/utils/","text":"\u00b6 Auxiliar functions check_func_inverse ( func , inv_func ) \u00b6 check that func and inv_func are indeed inverse of each other Source code in echoes/utils.py 21 22 23 24 25 26 27 28 def check_func_inverse ( func : Callable , inv_func : Callable ) -> None : \"\"\"check that func and inv_func are indeed inverse of each other\"\"\" x = np . linspace ( - 2 , 2 , 10 ) y = func ( x ) mismatch = np . where ( inv_func ( y ) != x )[ 0 ] assert np . isclose ( inv_func ( y ), x ) . all (), f \"function { inv_func . __name__ } is not the inverse of { func . __name__ } \" check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) \u00b6 Check shapes of W, W_in, W_fb Source code in echoes/utils.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ): \"\"\"Check shapes of W, W_in, W_fb\"\"\" assert W . shape [ 0 ] == W . shape [ 1 ], \"W must be square\" assert len ( W ) == n_reservoir , \"W does not match n_reservoir\" assert W_in . shape [ 0 ] == n_reservoir , \"W_in first dimension must equal n_reservoir\" assert ( W_in . shape [ 1 ] == n_inputs + 1 ), \"W_in second dimension must equal n_inputs + 1 (bias)\" if feedback : assert W_fb is not None , \"W_fb must be specified if feedback=True\" if W_fb is not None : assert ( W_fb . shape [ 0 ] == n_reservoir ), \"W_fb first dimension must equal n_reservoir\" assert ( W_fb . shape [ 1 ] == n_outputs ), \"W_fb second dimension must equal n_outputs\" check_model_params ( params ) \u00b6 check consistency of parameters, shapes, sensible warnings Source code in echoes/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def check_model_params ( params : Mapping ,) -> None : \"\"\"check consistency of parameters, shapes, sensible warnings\"\"\" W_in = params [ \"W_in_\" ] W = params [ \"W_\" ] W_fb = params [ \"W_fb_\" ] n_reservoir = params [ \"n_reservoir_\" ] n_inputs = params [ \"n_inputs_\" ] n_outputs = params [ \"n_outputs_\" ] feedback = params [ \"feedback\" ] input_scaling = params [ \"input_scaling\" ] input_shift = params [ \"input_shift\" ] check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) check_func_inverse ( params [ \"activation_out\" ], params [ \"inv_activation_out\" ]) check_sparsity ( params [ \"sparsity\" ]) check_input_scaling ( input_scaling , n_inputs ) check_input_shift ( input_scaling , n_inputs ) # Warnings if params [ \"leak_rate\" ] == 0 : warnings . warn ( \"leak_rate == 0 is total leakeage, you probably meant 1. See documentation.\" ) if ( params [ \"regression_method\" ] != \"ridge\" and params [ \"ridge_sample_weight\" ] is not None ): warnings . warn ( \"ridge_sample_weight will be ignored since regression_method is not ridge\" ) set_spectral_radius ( matrix , target_radius ) \u00b6 Rescale weights matrix to match target spectral radius Source code in echoes/utils.py 10 11 12 13 14 def set_spectral_radius ( matrix : np . ndarray , target_radius : float ) -> np . ndarray : \"\"\"Rescale weights matrix to match target spectral radius\"\"\" current_radius = np . max ( np . abs ( np . linalg . eigvals ( matrix ))) matrix *= target_radius / current_radius return matrix","title":"utils"},{"location":"api/utils/#echoes.utils","text":"Auxiliar functions","title":"echoes.utils"},{"location":"api/utils/#echoes.utils.check_func_inverse","text":"check that func and inv_func are indeed inverse of each other Source code in echoes/utils.py 21 22 23 24 25 26 27 28 def check_func_inverse ( func : Callable , inv_func : Callable ) -> None : \"\"\"check that func and inv_func are indeed inverse of each other\"\"\" x = np . linspace ( - 2 , 2 , 10 ) y = func ( x ) mismatch = np . where ( inv_func ( y ) != x )[ 0 ] assert np . isclose ( inv_func ( y ), x ) . all (), f \"function { inv_func . __name__ } is not the inverse of { func . __name__ } \"","title":"check_func_inverse()"},{"location":"api/utils/#echoes.utils.check_matrices_shapes","text":"Check shapes of W, W_in, W_fb Source code in echoes/utils.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ): \"\"\"Check shapes of W, W_in, W_fb\"\"\" assert W . shape [ 0 ] == W . shape [ 1 ], \"W must be square\" assert len ( W ) == n_reservoir , \"W does not match n_reservoir\" assert W_in . shape [ 0 ] == n_reservoir , \"W_in first dimension must equal n_reservoir\" assert ( W_in . shape [ 1 ] == n_inputs + 1 ), \"W_in second dimension must equal n_inputs + 1 (bias)\" if feedback : assert W_fb is not None , \"W_fb must be specified if feedback=True\" if W_fb is not None : assert ( W_fb . shape [ 0 ] == n_reservoir ), \"W_fb first dimension must equal n_reservoir\" assert ( W_fb . shape [ 1 ] == n_outputs ), \"W_fb second dimension must equal n_outputs\"","title":"check_matrices_shapes()"},{"location":"api/utils/#echoes.utils.check_model_params","text":"check consistency of parameters, shapes, sensible warnings Source code in echoes/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def check_model_params ( params : Mapping ,) -> None : \"\"\"check consistency of parameters, shapes, sensible warnings\"\"\" W_in = params [ \"W_in_\" ] W = params [ \"W_\" ] W_fb = params [ \"W_fb_\" ] n_reservoir = params [ \"n_reservoir_\" ] n_inputs = params [ \"n_inputs_\" ] n_outputs = params [ \"n_outputs_\" ] feedback = params [ \"feedback\" ] input_scaling = params [ \"input_scaling\" ] input_shift = params [ \"input_shift\" ] check_matrices_shapes ( W_in , W , W_fb , n_inputs , n_reservoir , n_outputs , feedback ) check_func_inverse ( params [ \"activation_out\" ], params [ \"inv_activation_out\" ]) check_sparsity ( params [ \"sparsity\" ]) check_input_scaling ( input_scaling , n_inputs ) check_input_shift ( input_scaling , n_inputs ) # Warnings if params [ \"leak_rate\" ] == 0 : warnings . warn ( \"leak_rate == 0 is total leakeage, you probably meant 1. See documentation.\" ) if ( params [ \"regression_method\" ] != \"ridge\" and params [ \"ridge_sample_weight\" ] is not None ): warnings . warn ( \"ridge_sample_weight will be ignored since regression_method is not ridge\" )","title":"check_model_params()"},{"location":"api/utils/#echoes.utils.set_spectral_radius","text":"Rescale weights matrix to match target spectral radius Source code in echoes/utils.py 10 11 12 13 14 def set_spectral_radius ( matrix : np . ndarray , target_radius : float ) -> np . ndarray : \"\"\"Rescale weights matrix to match target spectral radius\"\"\" current_radius = np . max ( np . abs ( np . linalg . eigvals ( matrix ))) matrix *= target_radius / current_radius return matrix","title":"set_spectral_radius()"},{"location":"examples/plot_generator_mackeyglass17/","text":"ESNGenerator example This example shows how to use Echo State Networks as pattern generator to produce a Mackey-Glass system . We try to predict the future steps of a chaotic time series. For testing, we use the Echo State Network in \"generative mode\", which means, we do not have any input (only the bias) and the output is fed back into the network for producing the next time step. Several parameter used in the example are arbitrary and even not so conventional (e.g., spectral radius > 1), just just for the sake of the example. Other constellations produce satisfactory results too, so feel free to play around with them. import numpy as np from matplotlib import pyplot as plt from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import plot_predicted_ts , set_mystyle set_mystyle () # optional, set aesthetics # Load and split data mackey_ts = load_mackeyglasst17 () n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( mackey_ts , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) esn = ESNGenerator ( n_steps = n_test_steps , n_reservoir = 200 , spectral_radius = 1.25 , leak_rate =. 4 , random_state = 42 , ) # Fit the model. Inputs is None because we only have the target time series esn . fit ( X = None , y = y_train ) y_pred = esn . predict () print ( \"test r2 score\" , r2_score ( y_test , y_pred )) # Plot training and test plt . figure ( figsize = ( 22 , 5 )) plt . plot ( mackey_ts [: n_total_steps ], 'steelblue' , linewidth = 5 , label = \"target system\" ) plt . plot ( esn . training_prediction_ , color = \"y\" , linewidth = 1 , label = \"training fit\" ) plt . plot ( range ( n_train_steps , n_total_steps ), y_pred , 'orange' , label = \"ESNGenerator\" ) plt . ylabel ( \"oscillator value\" ) plt . xlabel ( 'time' ) lo , hi = plt . ylim () plt . vlines ( n_train_steps , lo -. 05 , hi +. 05 , linestyles = '--' ) plt . legend ( fontsize = 'small' ) # Plot test alone plt . figure ( figsize = ( 22 , 5 )) plt . subplot ( 1 , 4 , ( 1 , 3 )) plt . title ( \"zoom into test\" ) plt . plot ( y_test , color = \"steelblue\" , label = \"target system\" , linewidth = 5.5 ) plt . xlabel ( 'time' ) plt . plot ( y_pred , linestyle = '--' , color = \"orange\" , linewidth = 2 , label = \"generative ESN\" ,) plt . ylabel ( \"oscillator\" ) plt . xlabel ( 'time' ) plt . legend ( fontsize = 'small' ) plt . subplot ( 1 , 4 , 4 ) plt . title ( r \"$W^ {out} $ weights distribution\" ) plt . xlabel ( 'weight' ) plt . ylabel ( 'frequency' ) plt . hist ( esn . W_out_ . flat ) plt . tight_layout (); test r2 score 0.4041090675570501","title":"ESNGenerator (Mackey-Glass)"},{"location":"examples/plot_regressor_sincos/","text":"ESNRegressor example This example shows a minimal example of Echo State Network applied to a regression problem. import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from echoes import ESNRegressor from echoes.plotting import set_mystyle set_mystyle () # optional: set aesthetics # Prepare synthetic data x = np . linspace ( 0 , 30 * np . pi , 1000 ) . reshape ( - 1 , 1 ) inputs = np . sin ( x ) + np . random . normal ( scale =. 1 , size = x . shape ) outputs = np . cos ( x ) + np . random . normal ( scale =. 1 , size = x . shape ) X_train , X_test , y_train , y_test = train_test_split ( inputs , outputs , test_size =. 3 , shuffle = False ) esn = ESNRegressor ( spectral_radius =. 95 , leak_rate =. 4 , n_transient = 100 , regression_method = \"pinv\" , random_state = 42 ) esn . fit ( X_train , y_train ) print ( \"training r2 score: \" , esn . score ( X_train , y_train )) print ( \"test r2 score: \" , esn . score ( X_test , y_test )) # Get prediction for plotting y_pred = esn . predict ( X_test ) plt . figure ( figsize = ( 15 , 4 )) plt . subplot ( 1 , 3 , ( 1 , 2 )) plt . plot ( y_test [ esn . n_transient :], label = 'target signal' , color = \"steelblue\" , linewidth = 5.5 ) plt . plot ( y_pred [ esn . n_transient :], label = 'predicted signal' , linestyle = '--' , color = \"orange\" , linewidth = 2 ,) plt . ylabel ( \"oscillator\" ) plt . xlabel ( 'time' ) plt . legend ( fontsize = ( \"small\" ), loc = 2 ) plt . subplot ( 1 , 3 , 3 ) plt . title ( r \"$W^ {out} $ weights distribution\" ) plt . xlabel ( 'weight' ) plt . ylabel ( 'frequency' ) plt . hist ( esn . W_out_ . flat ); training r2 score: 0.9840476267330117 test r2 score: 0.9733774695460478","title":"ESNRegressor (sin-cos)"},{"location":"examples/plot_reservoir_activity/","text":"Visualization of reservoir neurons activity \u00b6 This example shows how to access the time series of the reservoir neurons activity during the prediction phase. We use the ESNGenerator model as a pattern generator that learns to predict future steps of the chaotic time series Mackey-Glass Then we plot some randomly picked neurons from the reservoir. import numpy as np from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import ( set_mystyle , plot_reservoir_activity , plot_predicted_ts ) set_mystyle () # just aesthetics # Load data and define train/test length data = load_mackeyglasst17 () . reshape ( - 1 , 1 ) n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( data , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) # Instantiate model esn = ESNGenerator ( n_reservoir = 200 , n_steps = n_test_steps , spectral_radius = 1.25 , feedback = True , leak_rate =. 4 , regression_method = \"pinv\" , store_states_pred = True , # store states to plot later random_state = 42 , ) . fit ( None , y = y_train ) prediction = esn . predict () ax = plot_predicted_ts ( y_test , prediction , end = 500 , figsize = ( 12 , 3 ) ) # We can customize the plot ax . set_title ( \"My prediction\" ) ax . legend ( loc = 1 , fancybox = True , shadow = True ) # Pick 9 random neurons to plot neurons_to_plot = sorted ( np . random . randint ( 0 , esn . n_reservoir , size = 9 )) # This plots the activity and return the fig object for finetuning fig = plot_reservoir_activity ( esn , neurons_to_plot , pred = True , # plot activity during prediction end = 500 , figsize = ( 12 , 8 ), color = \"tab:orange\" , alpha =. 7 ) # Optional finetuning for i , ax in enumerate ( fig . axes ): if i % 2 == 0 : ax . set_title ( \"$ \\\\ bf{CUSTOM-TITLE}$\" )","title":"Plot Reservoir Activity"},{"location":"examples/plot_reservoir_activity/#visualization-of-reservoir-neurons-activity","text":"This example shows how to access the time series of the reservoir neurons activity during the prediction phase. We use the ESNGenerator model as a pattern generator that learns to predict future steps of the chaotic time series Mackey-Glass Then we plot some randomly picked neurons from the reservoir. import numpy as np from sklearn.model_selection import train_test_split from echoes import ESNGenerator from echoes.datasets import load_mackeyglasst17 from echoes.plotting import ( set_mystyle , plot_reservoir_activity , plot_predicted_ts ) set_mystyle () # just aesthetics # Load data and define train/test length data = load_mackeyglasst17 () . reshape ( - 1 , 1 ) n_train_steps , n_test_steps = 2000 , 2000 n_total_steps = n_train_steps + n_test_steps y_train , y_test = train_test_split ( data , train_size = n_train_steps , test_size = n_test_steps , shuffle = False ) # Instantiate model esn = ESNGenerator ( n_reservoir = 200 , n_steps = n_test_steps , spectral_radius = 1.25 , feedback = True , leak_rate =. 4 , regression_method = \"pinv\" , store_states_pred = True , # store states to plot later random_state = 42 , ) . fit ( None , y = y_train ) prediction = esn . predict () ax = plot_predicted_ts ( y_test , prediction , end = 500 , figsize = ( 12 , 3 ) ) # We can customize the plot ax . set_title ( \"My prediction\" ) ax . legend ( loc = 1 , fancybox = True , shadow = True ) # Pick 9 random neurons to plot neurons_to_plot = sorted ( np . random . randint ( 0 , esn . n_reservoir , size = 9 )) # This plots the activity and return the fig object for finetuning fig = plot_reservoir_activity ( esn , neurons_to_plot , pred = True , # plot activity during prediction end = 500 , figsize = ( 12 , 8 ), color = \"tab:orange\" , alpha =. 7 ) # Optional finetuning for i , ax in enumerate ( fig . axes ): if i % 2 == 0 : ax . set_title ( \"$ \\\\ bf{CUSTOM-TITLE}$\" )","title":"Visualization of reservoir neurons activity"}]}