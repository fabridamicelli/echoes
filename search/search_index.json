{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"echoes","text":"<p>High level API for machine learning with Echo State Networks (ESN) \u2013 work in progress!.</p> <p>Check out the examples for a quick start and What are Echo State Networks? section for a little intro about Echo State Networks.</p> <p>The library is scikit-learn compatible, thus you can directly use sklearn utils, such as <code>GridSearchCV</code>. Moreover, it tries to stick to the intuitions that you might bring from using sklearn. For example, models can be initialized without passing arguments (but kwargs are enforced if you choose to pass any); attributes generated during fitting are stored with trailing underscore; etc.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install the package via pip</p> <pre><code>pip install echoes\n</code></pre>"},{"location":"#citing","title":"Citing","text":"<p>If you find echoes useful for a publication, then please use the following BibTeX to cite it:</p> <pre><code>@misc{echoes,\n  author = {Damicelli, Fabrizio},\n  title = {echoes: Echo State Networks with Python},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/fabridamicelli/echoes}},\n}\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>numpy</li> <li>numba</li> <li>scikit-learn</li> <li>pandas</li> <li>matplotlib</li> <li>seaborn </li> <li>tests: mypy, pytest </li> </ul> <p>The code has been tested with Python 3.7 on Ubuntu 16.04/18.04.</p>"},{"location":"#tests","title":"Tests","text":"<p>Run tests with </p> <pre><code>make test\n</code></pre>"},{"location":"api/ESNGenerator/","title":"ESNGenerator","text":"<p>               Bases: <code>ESNBase</code>, <code>MultiOutputMixin</code>, <code>RegressorMixin</code></p> <p>The number of inputs (n_inputs) is always 1 and n_outputs is infered from passed data. It uses always feedback, so that is not a parameter anymore (always True).</p> <p>Parameters:</p> Name Type Description Default <code>n_steps</code> <code>int</code> <p>int, default=100 Number of steps to generate pattern (used by predict method).</p> <code>100</code> <code>n_reservoir</code> <code>int</code> <p>int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed.</p> <code>100</code> <code>W</code> <code>ndarray | None</code> <p>np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed.</p> <code>None</code> <code>spectral_radius</code> <code>float</code> <p>float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W).</p> <code>0.99</code> <code>W_in</code> <code>ndarray | None</code> <p>np.ndarray of shape (n_reservoir, 1+n_inputs) (1-&gt;bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used.</p> <code>None</code> <code>W_fb</code> <code>ndarray | None</code> <p>np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback.</p> <code>None</code> <code>sparsity</code> <code>float</code> <p>float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored.</p> <code>0.0</code> <code>noise</code> <code>float</code> <p>float, optional, default=0 Scaling factor of the (uniform) noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5.</p> <code>0.0</code> <code>leak_rate</code> <code>float</code> <p>float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage.</p> <code>1.0</code> <code>bias</code> <code>float | ndarray</code> <p>int, float or np.ndarray, optional, default=1 Value of the bias neuron, injected at each time to the reservoir neurons. If int or float, all neurons receive the same. If np.ndarray is must be of length n_reservoir.</p> <code>1.0</code> <code>activation</code> <code>Callable</code> <p>function (numba jitted), optional, default=tanh Non-linear activation function applied to the neurons at each step. For numba acceleration, it must be a jitted function. Basic activation functions as tanh, sigmoid, relu are already available in echoe.utils. Either use those or write a custom one decorated with numba njit.</p> <code>tanh</code> <code>activation_out</code> <code>Callable</code> <p>function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed.</p> <code>identity</code> <code>fit_only_states</code> <code>bool</code> <p>bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase.</p> <code>False</code> <code>regression_method</code> <code>str</code> <p>str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used.</p> <code>'pinv'</code> <code>ridge_alpha</code> <code>float</code> <p>float, ndarray of shape (n_outputs,), default=None Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details.</p> <code>1.0</code> <code>ridge_fit_intercept</code> <code>bool</code> <p>bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details.</p> <code>False</code> <code>ridge_max_iter</code> <code>Union[int, None]</code> <p>int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details.</p> <code>None</code> <code>ridge_tol</code> <code>float</code> <p>float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details.</p> <code>0.001</code> <code>ridge_solver</code> <code>str</code> <p>str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details.</p> <code>'auto'</code> <code>ridge_sample_weight</code> <code>float | ndarray | None</code> <p>float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details.</p> <code>None</code> <code>n_transient</code> <code>int</code> <p>int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak.</p> <code>0</code> <code>random_state</code> <p>int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn:   If int, random_state is the seed used by the random number generator;   If RandomState instance, random_state is the random number generator;   If None, the random number generator is the RandomState instance used   by <code>np.random</code>.</p> <code>None</code> <code>store_states_train</code> <code>bool</code> <p>bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_.</p> <code>False</code> <code>store_states_pred</code> <code>bool</code> <p>bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_.</p> <code>False</code>"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator--attributes","title":"Attributes:","text":"<pre><code>- W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1).\n    Outgoing weights after fitting linear regression model to predict outputs.\n- training_prediction_: array of shape (n_samples, n_outputs).\n    Predicted output on training data.\n- states_train_: array of shape (n_samples, n_reservoir), default False.\n    If store_states_train is True, states matrix is stored for visualizing\n    reservoir neurons activity during training.\n- states_pred_: array of shape (n_samples, n_reservoir), default False.\n    If store_states_pred is True, states matrix is stored for visualizing\n    reservoir neurons activity during prediction (test).\n</code></pre> Source code in <code>echoes/esn/_generator.py</code> <pre><code>class ESNGenerator(ESNBase, MultiOutputMixin, RegressorMixin):\n    \"\"\"\n    The number of inputs (n_inputs) is always 1 and n_outputs is infered from passed\n    data.\n    It uses always feedback, so that is not a parameter anymore (always True).\n\n    Arguments:\n        n_steps: int, default=100\n            Number of steps to generate pattern (used by predict method).\n        n_reservoir: int, optional, default=100\n            Number of reservoir neurons. Only used if W is not passed.\n            If W is passed, n_reservoir gets overwritten with len(W).\n            Either n_reservoir or W must be passed.\n        W: np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None\n            Reservoir weights matrix. If None, random weights are used (uniformly\n            distributed around 0, ie., in [-0.5, 0.5).\n            Be careful with the distribution of W values. Wrong W initialization\n            might drastically affect test performance (even with reasonable good\n            training fit).\n            Spectral radius will be adjusted in all cases.\n            Either n_reservoir or W must be passed.\n        spectral_radius: float, default=.99\n            Spectral radius of the reservoir weights matrix (W).\n            Spectral radius will be adjusted in all cases (also with user specified W).\n        W_in: np.ndarray of shape (n_reservoir, 1+n_inputs) (1-&gt;bias), optional,\n            default None.\n            Input weights matrix by which input signal is multiplied.\n            If None, random weights are used.\n        W_fb: np.ndarray of shape(n_reservoir, n_outputs), optional, default None.\n            Feedback weights matrix by which feedback is multiplied in case of feedback.\n        sparsity: float, optional, default=0\n            Proportion of the reservoir matrix weights forced to be zero.\n            Note that with default W (centered around 0), the actual sparsity will\n            be slightly more than the specified.\n            If W is passed, sparsity will be ignored.\n        noise: float, optional, default=0\n            Scaling factor of the (uniform) noise input added to neurons at each step.\n            This is used for regularization purposes and should typically be\n            very small, e.g. 0.0001 or 1e-5.\n        leak_rate: float, optional, default=1\n            Leaking rate applied to the neurons at each step.\n            Default is 1, which is no leaking. 0 would be total leakeage.\n        bias: int, float or np.ndarray, optional, default=1\n            Value of the bias neuron, injected at each time to the reservoir neurons.\n            If int or float, all neurons receive the same.\n            If np.ndarray is must be of length n_reservoir.\n        activation: function (numba jitted), optional, default=tanh\n            Non-linear activation function applied to the neurons at each step.\n            For numba acceleration, it must be a jitted function.\n            Basic activation functions as tanh, sigmoid, relu are already available\n            in echoe.utils. Either use those or write a custom one decorated with\n            numba njit.\n        activation_out: function, optional, default=identity\n            Activation function applied to the outputs. In other words, it is assumed\n            that targets = f(outputs). So the output produced must be transformed.\n        fit_only_states: bool,default=False\n            If True, outgoing weights (W_out) are computed fitting only the reservoir\n            states. Inputs and bias are still use to drive reservoir activity, but\n            ignored for fitting W_out, both in the training and prediction phase.\n        regression_method: str, optional, default \"pinv\" (pseudoinverse).\n            Method to solve the linear regression to find out outgoing weights.\n            One of [\"pinv\", \"ridge\"].\n            If \"ridge\", ridge_* parameters will be used.\n        ridge_alpha: float, ndarray of shape (n_outputs,), default=None\n            Regularization coefficient used for Ridge regression.\n            Larger values specify stronger regularization.\n            If an array is passed, penalties are assumed to be specific to the targets.\n            Hence they must correspond in number.\n            Default is None to make sure one deliberately sets this since it is\n            a crucial parameter. See sklearn Ridge documentation for details.\n        ridge_fit_intercept: bool, optional, default=False\n            If True, intercept is fit in Ridge regression. Default False.\n            See sklearn Ridge documentation for details.\n        ridge_max_iter: int, default=None\n            Maximum number of iterations for conjugate gradient solver.\n            See sklearn Ridge documentation for details.\n        ridge_tol: float, default=1e-3\n            Precision of the solution.\n            See sklearn Ridge documentation for details.\n        ridge_solver: str, optional, default=\"auto\"\n            Solver to use in the Ridge regression.\n            One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"].\n            See sklearn Ridge documentation for details.\n        ridge_sample_weight: float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample.\n            If given a float, every sample will have the same weight.\n            See sklearn Ridge documentation for details.\n        n_transient: int, optional, default=0\n            Number of activity initial steps removed (not considered for training)\n            in order to avoid initial instabilities.\n            Default is 0, but this is something one definitely might want to tweak.\n        random_state : int, RandomState instance, default=None\n            The seed of the pseudo random number generator used to generate weight\n            matrices, to generate noise inyected to reservoir neurons (regularization)\n            and it is passed to the ridge solver in case regression_method=ridge.\n            From sklearn:\n              If int, random_state is the seed used by the random number generator;\n              If RandomState instance, random_state is the random number generator;\n              If None, the random number generator is the RandomState instance used\n              by `np.random`.\n        store_states_train: bool, optional, default=False\n            If True, time series series of reservoir neurons during training are stored\n            in the object attribute states_train_.\n        store_states_pred: bool, optional, default=False\n            If True, time series series of reservoir neurons during prediction are\n            stored in the object attribute states_pred_.\n\n    ### Attributes:\n        - W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1).\n            Outgoing weights after fitting linear regression model to predict outputs.\n        - training_prediction_: array of shape (n_samples, n_outputs).\n            Predicted output on training data.\n        - states_train_: array of shape (n_samples, n_reservoir), default False.\n            If store_states_train is True, states matrix is stored for visualizing\n            reservoir neurons activity during training.\n        - states_pred_: array of shape (n_samples, n_reservoir), default False.\n            If store_states_pred is True, states matrix is stored for visualizing\n            reservoir neurons activity during prediction (test).\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        n_steps: int = 100,\n        n_reservoir: int = 100,\n        W: np.ndarray | None = None,\n        spectral_radius: float = 0.99,\n        W_in: np.ndarray | None = None,\n        W_fb: np.ndarray | None = None,\n        sparsity: float = 0.0,\n        noise: float = 0.0,\n        leak_rate: float = 1.0,\n        bias: float | np.ndarray = 1.0,\n        input_scaling: float | np.ndarray | None = None,\n        input_shift: float | np.ndarray | None = None,\n        activation: Callable = tanh,\n        activation_out: Callable = identity,\n        fit_only_states: bool = False,\n        regression_method: str = \"pinv\",\n        ridge_alpha: float = 1.0,\n        ridge_fit_intercept: bool = False,\n        ridge_max_iter: Union[int, None] = None,\n        ridge_tol: float = 1e-3,\n        ridge_solver: str = \"auto\",\n        ridge_sample_weight: float | np.ndarray | None = None,\n        n_transient: int = 0,\n        store_states_train: bool = False,\n        store_states_pred: bool = False,\n        random_state: int | np.random.RandomState | None = None,\n    ) -&gt; None:\n        self.n_steps = n_steps\n        self.n_reservoir = n_reservoir\n        self.spectral_radius = spectral_radius\n        self.W = W\n        self.W_in = W_in\n        self.W_fb = W_fb\n        self.sparsity = sparsity\n        self.noise = noise\n        self.leak_rate = leak_rate\n        self.bias = bias\n        self.input_scaling = input_scaling\n        self.input_shift = input_shift\n        self.activation = activation\n        self.activation_out = activation_out\n        self.fit_only_states = fit_only_states\n        self.n_transient = n_transient\n        self.store_states_train = store_states_train\n        self.store_states_pred = store_states_pred\n        self.regression_method = regression_method\n        self.ridge_alpha = ridge_alpha\n        self.ridge_fit_intercept = ridge_fit_intercept\n        self.ridge_max_iter = ridge_max_iter\n        self.ridge_tol = ridge_tol\n        self.ridge_solver = ridge_solver\n        self.ridge_sample_weight = ridge_sample_weight\n        self.random_state = random_state\n        self.feedback = True  # Generator uses feedback always\n\n    def fit(self, X=None, y=None) -&gt; \"ESNGenerator\":\n        \"\"\"\n        Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later\n        prediction.\n        Bias is appended automatically to the inputs.\n\n        Arguments:\n            X: None, always ignored. Argument kept only for API consistency.\n                It is ignored as only the target sequence matters (outputs).\n                A sequence of zeros will be fed in - matching the len(outputs) as\n                initial condition.\n            y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs),\n                default=None.\n                Target variable.\n\n        Returns:\n            self: returns an instance of self.\n        \"\"\"\n        if X is not None:\n            raise ValueError(\n                \"X must be None, ESNGenerator takes no X for prediction.\"\n                \"The parameter is kept only for API consistency here.\"\n            )\n        y = check_array(y, ensure_2d=False, dtype=\"numeric\")\n        self._dtype_ = y.dtype\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n        outputs = y\n\n        # Initialize matrices and random state\n        self.random_state_ = check_random_state(self.random_state)\n        # Pattern generation takes no input, thus hardcode for later\n        # construction of matrices\n        self.n_inputs_ = 1\n        self.n_reservoir_ = len(self.W) if self.W is not None else self.n_reservoir\n        self.n_outputs_ = outputs.shape[1]\n        self.W_in_ = self._init_incoming_weights()\n        self.W_ = self._init_reservoir_weights()\n        self.W_fb_ = self._init_feedback_weights()\n\n        check_model_params(self.__dict__)\n\n        # Make inputs zero\n        inputs = np.zeros(shape=(outputs.shape[0], self.n_inputs_), dtype=self._dtype_)\n\n        check_consistent_length(inputs, outputs)  # sanity check\n\n        # Initialize reservoir model\n        self.reservoir_ = self._init_reservoir_neurons()\n\n        states = self.reservoir_.harvest_states(inputs, outputs, initial_state=None)\n\n        # Extend states matrix with inputs; i.e., make [h(t); x(t)]\n        full_states = states if self.fit_only_states else np.hstack((states, inputs))\n\n        # Solve for W_out using full states and outputs, excluding transient\n        self.W_out_ = self._solve_W_out(\n            full_states[self.n_transient :, :], outputs[self.n_transient :, :]\n        )\n        # Predict on training set (including the pass through the output nonlinearity)\n        self.training_prediction_ = self.activation_out(full_states @ self.W_out_.T)\n\n        # Keep last state for later\n        self.last_state_ = states[-1, :]\n        self.last_input_ = inputs[-1, :]\n        self.last_output_ = outputs[-1, :]\n\n        # Store reservoir activity\n        if self.store_states_train:\n            self.states_train_ = states\n        return self\n\n    def predict(self, X=None) -&gt; np.ndarray:\n        \"\"\"\n        Last training state/input/output is used as initial test\n        state/input/output and at each step the output of the network is reinjected\n        as input for next prediction, thus no inputs are needed for prediction.\n\n        Arguments:\n            X: None, always ignored, API consistency\n\n        Returns:\n            outputs: 2D np.ndarray of shape (n_steps, n_outputs)\n             Predicted outputs.\n        \"\"\"\n        check_is_fitted(self)\n        if X is not None:\n            raise ValueError(\n                \"X must be None, ESNGenerator takes no X for prediction.\"\n                \"The parameter is kept only for API consistency here.\"\n            )\n\n        # TODO: add test\n        assert self.n_steps &gt;= 1, \"n_steps must be &gt;= 1\"\n        assert self.n_inputs_ == 1, \"n_inputs must be == 1\"\n\n        # Initialize predictions: begin with last state as first state\n        inputs = np.zeros(shape=(self.n_steps, self.n_inputs_), dtype=self._dtype_)\n        inputs = np.vstack([self.last_input_, inputs])\n        states = np.vstack(\n            [\n                self.last_state_,\n                np.zeros((self.n_steps, self.n_reservoir_), dtype=self._dtype_),\n            ]\n        )\n        outputs = np.vstack(\n            [\n                self.last_output_,\n                np.zeros((self.n_steps, self.n_outputs_), dtype=self._dtype_),\n            ]\n        )\n\n        check_consistent_length(inputs, outputs)  # sanity check\n\n        # Go through samples (steps) and predict for each of them\n        for t in range(1, outputs.shape[0]):\n            states[t, :] = self.reservoir_.update_state(\n                state_t=states[t - 1, :],\n                X_t=inputs[t, :],\n                y_t=outputs[t - 1, :],\n            )\n            if self.fit_only_states:\n                full_states = states[t, :]\n            else:\n                full_states = np.concatenate([states[t, :], inputs[t, :]])\n            # Predict\n            outputs[t, :] = self.activation_out(self.W_out_ @ full_states)\n\n            # TODO: check: shoud we Update last_{input, states, outputs}_?\n            # That would imply that succesively calls predict() render potentially\n            # different results because we are updating the inner state.\n            # Keep last state for later\n            self.last_state_ = states[-1, :]\n            self.last_input_ = inputs[-1, :]\n            self.last_output_ = outputs[-1, :]\n\n        # Store reservoir activity\n        if self.store_states_pred:\n            self.states_pred_ = states[1:, :]  # discard first step (comes from fitting)\n\n        return outputs[1:, :]  # discard initial step (comes from training)\n\n    def score(self, X=None, y=None, sample_weight=None):\n        \"\"\"\n        Wrapper around sklearn r2_score with kwargs.\n\n        From sklearn:\n          R^2 (coefficient of determination) regression score function.\n          Best possible score is 1.0 and it can be negative (because the model can be\n          arbitrarily worse).\n          A constant model that always predicts the expected value of y,\n          disregarding the input features, would get a R^2 score of 0.0.\n\n        Arguments:\n            X: None\n                Not used, present for API consistency.\n                Generative ESN predicts purely based on its generative outputs.\n            y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs)\n                Target sequence, true values of the outputs.\n            sample_weight: array-like of shape (n_samples,), default=None\n                Sample weights.\n\n        Returns:\n            score: float\n                R2 score\n        \"\"\"\n        return r2_score(y, self.predict(), sample_weight=sample_weight)\n</code></pre>"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.fit","title":"<code>fit(X=None, y=None)</code>","text":"<p>Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>None, always ignored. Argument kept only for API consistency. It is ignored as only the target sequence matters (outputs). A sequence of zeros will be fed in - matching the len(outputs) as initial condition.</p> <code>None</code> <code>y</code> <p>2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs), default=None. Target variable.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>'ESNGenerator'</code> <p>returns an instance of self.</p> Source code in <code>echoes/esn/_generator.py</code> <pre><code>def fit(self, X=None, y=None) -&gt; \"ESNGenerator\":\n    \"\"\"\n    Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later\n    prediction.\n    Bias is appended automatically to the inputs.\n\n    Arguments:\n        X: None, always ignored. Argument kept only for API consistency.\n            It is ignored as only the target sequence matters (outputs).\n            A sequence of zeros will be fed in - matching the len(outputs) as\n            initial condition.\n        y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs),\n            default=None.\n            Target variable.\n\n    Returns:\n        self: returns an instance of self.\n    \"\"\"\n    if X is not None:\n        raise ValueError(\n            \"X must be None, ESNGenerator takes no X for prediction.\"\n            \"The parameter is kept only for API consistency here.\"\n        )\n    y = check_array(y, ensure_2d=False, dtype=\"numeric\")\n    self._dtype_ = y.dtype\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    outputs = y\n\n    # Initialize matrices and random state\n    self.random_state_ = check_random_state(self.random_state)\n    # Pattern generation takes no input, thus hardcode for later\n    # construction of matrices\n    self.n_inputs_ = 1\n    self.n_reservoir_ = len(self.W) if self.W is not None else self.n_reservoir\n    self.n_outputs_ = outputs.shape[1]\n    self.W_in_ = self._init_incoming_weights()\n    self.W_ = self._init_reservoir_weights()\n    self.W_fb_ = self._init_feedback_weights()\n\n    check_model_params(self.__dict__)\n\n    # Make inputs zero\n    inputs = np.zeros(shape=(outputs.shape[0], self.n_inputs_), dtype=self._dtype_)\n\n    check_consistent_length(inputs, outputs)  # sanity check\n\n    # Initialize reservoir model\n    self.reservoir_ = self._init_reservoir_neurons()\n\n    states = self.reservoir_.harvest_states(inputs, outputs, initial_state=None)\n\n    # Extend states matrix with inputs; i.e., make [h(t); x(t)]\n    full_states = states if self.fit_only_states else np.hstack((states, inputs))\n\n    # Solve for W_out using full states and outputs, excluding transient\n    self.W_out_ = self._solve_W_out(\n        full_states[self.n_transient :, :], outputs[self.n_transient :, :]\n    )\n    # Predict on training set (including the pass through the output nonlinearity)\n    self.training_prediction_ = self.activation_out(full_states @ self.W_out_.T)\n\n    # Keep last state for later\n    self.last_state_ = states[-1, :]\n    self.last_input_ = inputs[-1, :]\n    self.last_output_ = outputs[-1, :]\n\n    # Store reservoir activity\n    if self.store_states_train:\n        self.states_train_ = states\n    return self\n</code></pre>"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.predict","title":"<code>predict(X=None)</code>","text":"<p>Last training state/input/output is used as initial test state/input/output and at each step the output of the network is reinjected as input for next prediction, thus no inputs are needed for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>None, always ignored, API consistency</p> <code>None</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>ndarray</code> <p>2D np.ndarray of shape (n_steps, n_outputs) Predicted outputs.</p> Source code in <code>echoes/esn/_generator.py</code> <pre><code>def predict(self, X=None) -&gt; np.ndarray:\n    \"\"\"\n    Last training state/input/output is used as initial test\n    state/input/output and at each step the output of the network is reinjected\n    as input for next prediction, thus no inputs are needed for prediction.\n\n    Arguments:\n        X: None, always ignored, API consistency\n\n    Returns:\n        outputs: 2D np.ndarray of shape (n_steps, n_outputs)\n         Predicted outputs.\n    \"\"\"\n    check_is_fitted(self)\n    if X is not None:\n        raise ValueError(\n            \"X must be None, ESNGenerator takes no X for prediction.\"\n            \"The parameter is kept only for API consistency here.\"\n        )\n\n    # TODO: add test\n    assert self.n_steps &gt;= 1, \"n_steps must be &gt;= 1\"\n    assert self.n_inputs_ == 1, \"n_inputs must be == 1\"\n\n    # Initialize predictions: begin with last state as first state\n    inputs = np.zeros(shape=(self.n_steps, self.n_inputs_), dtype=self._dtype_)\n    inputs = np.vstack([self.last_input_, inputs])\n    states = np.vstack(\n        [\n            self.last_state_,\n            np.zeros((self.n_steps, self.n_reservoir_), dtype=self._dtype_),\n        ]\n    )\n    outputs = np.vstack(\n        [\n            self.last_output_,\n            np.zeros((self.n_steps, self.n_outputs_), dtype=self._dtype_),\n        ]\n    )\n\n    check_consistent_length(inputs, outputs)  # sanity check\n\n    # Go through samples (steps) and predict for each of them\n    for t in range(1, outputs.shape[0]):\n        states[t, :] = self.reservoir_.update_state(\n            state_t=states[t - 1, :],\n            X_t=inputs[t, :],\n            y_t=outputs[t - 1, :],\n        )\n        if self.fit_only_states:\n            full_states = states[t, :]\n        else:\n            full_states = np.concatenate([states[t, :], inputs[t, :]])\n        # Predict\n        outputs[t, :] = self.activation_out(self.W_out_ @ full_states)\n\n        # TODO: check: shoud we Update last_{input, states, outputs}_?\n        # That would imply that succesively calls predict() render potentially\n        # different results because we are updating the inner state.\n        # Keep last state for later\n        self.last_state_ = states[-1, :]\n        self.last_input_ = inputs[-1, :]\n        self.last_output_ = outputs[-1, :]\n\n    # Store reservoir activity\n    if self.store_states_pred:\n        self.states_pred_ = states[1:, :]  # discard first step (comes from fitting)\n\n    return outputs[1:, :]  # discard initial step (comes from training)\n</code></pre>"},{"location":"api/ESNGenerator/#echoes.esn._generator.ESNGenerator.score","title":"<code>score(X=None, y=None, sample_weight=None)</code>","text":"<p>Wrapper around sklearn r2_score with kwargs.</p> From sklearn <p>R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>None Not used, present for API consistency. Generative ESN predicts purely based on its generative outputs.</p> <code>None</code> <code>y</code> <p>2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs) Target sequence, true values of the outputs.</p> <code>None</code> <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <p>float R2 score</p> Source code in <code>echoes/esn/_generator.py</code> <pre><code>def score(self, X=None, y=None, sample_weight=None):\n    \"\"\"\n    Wrapper around sklearn r2_score with kwargs.\n\n    From sklearn:\n      R^2 (coefficient of determination) regression score function.\n      Best possible score is 1.0 and it can be negative (because the model can be\n      arbitrarily worse).\n      A constant model that always predicts the expected value of y,\n      disregarding the input features, would get a R^2 score of 0.0.\n\n    Arguments:\n        X: None\n            Not used, present for API consistency.\n            Generative ESN predicts purely based on its generative outputs.\n        y: 2D np.ndarray of shape (n_samples, ) or (n_samples, n_outputs)\n            Target sequence, true values of the outputs.\n        sample_weight: array-like of shape (n_samples,), default=None\n            Sample weights.\n\n    Returns:\n        score: float\n            R2 score\n    \"\"\"\n    return r2_score(y, self.predict(), sample_weight=sample_weight)\n</code></pre>"},{"location":"api/ESNRegressor/","title":"ESNRegressor","text":"<p>               Bases: <code>ESNBase</code>, <code>MultiOutputMixin</code>, <code>RegressorMixin</code></p> <p>Number of input and output neurons are infered from passed data.</p> <p>Parameters:</p> Name Type Description Default <code>n_reservoir</code> <code>int</code> <p>int, optional, default=100 Number of reservoir neurons. Only used if W is not passed. If W is passed, n_reservoir gets overwritten with len(W). Either n_reservoir or W must be passed.</p> <code>100</code> <code>W</code> <code>ndarray | None</code> <p>np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None Reservoir weights matrix. If None, random weights are used (uniformly distributed around 0, ie., in [-0.5, 0.5). Be careful with the distribution of W values. Wrong W initialization might drastically affect test performance (even with reasonable good training fit). Spectral radius will be adjusted in all cases. Either n_reservoir or W must be passed.</p> <code>None</code> <code>spectral_radius</code> <code>float</code> <p>float, default=.99 Spectral radius of the reservoir weights matrix (W). Spectral radius will be adjusted in all cases (also with user specified W).</p> <code>0.99</code> <code>W_in</code> <code>ndarray | None</code> <p>np.ndarray of shape (n_reservoir, 1+n_inputs) (1-&gt;bias), optional, default None. Input weights matrix by which input signal is multiplied. If None, random weights are used.</p> <code>None</code> <code>W_fb</code> <code>ndarray | None</code> <p>np.ndarray of shape(n_reservoir, n_outputs), optional, default None. Feedback weights matrix by which feedback is multiplied in case of feedback.</p> <code>None</code> <code>sparsity</code> <code>float</code> <p>float, optional, default=0 Proportion of the reservoir matrix weights forced to be zero. Note that with default W (centered around 0), the actual sparsity will be slightly more than the specified. If W is passed, sparsity will be ignored.</p> <code>0</code> <code>noise</code> <code>float</code> <p>float, optional, default=0 Scaling factor of the (uniform) noise input added to neurons at each step. This is used for regularization purposes and should typically be very small, e.g. 0.0001 or 1e-5.</p> <code>0</code> <code>leak_rate</code> <code>float</code> <p>float, optional, default=1 Leaking rate applied to the neurons at each step. Default is 1, which is no leaking. 0 would be total leakeage.</p> <code>1</code> <code>bias</code> <code>float | ndarray</code> <p>int, float or np.ndarray, optional, default=1 Value of the bias neuron, injected at each time to the reservoir neurons. If int or float, all neurons receive the same. If np.ndarray is must be of length n_reservoir.</p> <code>1</code> <code>input_scaling</code> <code>float | ndarray | None</code> <p>float or np.ndarray of length n_inputs, default=None Scalar to multiply each input before feeding it to the network. If float, all inputs get multiplied by same value. If array, it must match n_inputs length (X.shape[1]), specifying the scaling factor for each input.</p> <code>None</code> <code>input_shift</code> <code>float | ndarray | None</code> <p>float or np.ndarray of length n_inputs, default=None Scalar to add to each input before feeding it to the network. If float, multiplied same value is added to all inputs. If array, it must match n_inputs length (X.shape[1]), specifying the value to add to each input.</p> <code>None</code> <code>feedback</code> <code>bool</code> <p>bool, optional, default=False If True, the reservoir also receives the outout signal as input.</p> <code>False</code> <code>activation</code> <code>Callable</code> <p>function (numba jitted), optional, default=tanh Non-linear activation function applied to the neurons at each step. For numba acceleration, it must be a jitted function. Basic activation functions as tanh, sigmoid, relu are already available in echoe.utils. Either use those or write a custom one decorated with numba njit.</p> <code>tanh</code> <code>activation_out</code> <code>Callable</code> <p>function, optional, default=identity Activation function applied to the outputs. In other words, it is assumed that targets = f(outputs). So the output produced must be transformed.</p> <code>identity</code> <code>fit_only_states</code> <code>bool</code> <p>bool,default=False If True, outgoing weights (W_out) are computed fitting only the reservoir states. Inputs and bias are still use to drive reservoir activity, but ignored for fitting W_out, both in the training and prediction phase.</p> <code>False</code> <code>regression_method</code> <code>str</code> <p>str, optional, default \"pinv\" (pseudoinverse). Method to solve the linear regression to find out outgoing weights. One of [\"pinv\", \"ridge\"]. If \"ridge\", ridge_* parameters will be used.</p> <code>'pinv'</code> <code>ridge_alpha</code> <code>float</code> <p>float, ndarray of shape (n_outputs,), default=1 Regularization coefficient used for Ridge regression. Larger values specify stronger regularization. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. Default is None to make sure one deliberately sets this since it is a crucial parameter. See sklearn Ridge documentation for details.</p> <code>1</code> <code>ridge_fit_intercept</code> <code>bool</code> <p>bool, optional, default=False If True, intercept is fit in Ridge regression. Default False. See sklearn Ridge documentation for details.</p> <code>False</code> <code>ridge_max_iter</code> <code>int | None</code> <p>int, default=None Maximum number of iterations for conjugate gradient solver. See sklearn Ridge documentation for details.</p> <code>None</code> <code>ridge_tol</code> <code>float</code> <p>float, default=1e-3 Precision of the solution. See sklearn Ridge documentation for details.</p> <code>0.001</code> <code>ridge_solver</code> <code>str</code> <p>str, optional, default=\"auto\" Solver to use in the Ridge regression. One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"]. See sklearn Ridge documentation for details.</p> <code>'auto'</code> <code>ridge_sample_weight</code> <code>float | ndarray | None</code> <p>float or ndarray of shape (n_samples,), default=None Individual weights for each sample. If given a float, every sample will have the same weight. See sklearn Ridge documentation for details.</p> <code>None</code> <code>n_transient</code> <code>int</code> <p>int, optional, default=0 Number of activity initial steps removed (not considered for training) in order to avoid initial instabilities. Default is 0, but this is something one definitely might want to tweak.</p> <code>0</code> <code>random_state</code> <p>int, RandomState instance, default=None The seed of the pseudo random number generator used to generate weight matrices, to generate noise inyected to reservoir neurons (regularization) and it is passed to the ridge solver in case regression_method=ridge. From sklearn:   If int, random_state is the seed used by the random number generator;   If RandomState instance, random_state is the random number generator;   If None, the random number generator is the RandomState instance used   by <code>np.random</code>.</p> <code>None</code> <code>store_states_train</code> <code>bool</code> <p>bool, optional, default=False If True, time series series of reservoir neurons during training are stored in the object attribute states_train_.</p> <code>False</code> <code>store_states_pred</code> <code>bool</code> <p>bool, optional, default=False If True, time series series of reservoir neurons during prediction are stored in the object attribute states_pred_.</p> <code>False</code>"},{"location":"api/ESNRegressor/#echoes.ESNRegressor--attributes","title":"Attributes:","text":"<pre><code>- W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1)\n     Outgoing weights after fitting linear regression model to predict outputs.\n- training_prediction_: array of shape (n_samples, n_outputs)\n     Predicted output on training data.\n- states_train_: array of shape (n_samples, n_reservoir), default False.\n     If store_states_train is True, states matrix is stored for visualizing\n     reservoir neurons activity during training.\n- states_pred_: array of shape (n_samples, n_reservoir), default False.\n     If store_states_pred is True, states matrix is stored for visualizing\n     reservoir neurons activity during prediction (test).\n</code></pre> Source code in <code>echoes/esn/_regressor.py</code> <pre><code>class ESNRegressor(ESNBase, MultiOutputMixin, RegressorMixin):\n    \"\"\"\n    Number of input and output neurons are infered from passed data.\n\n    Arguments:\n        n_reservoir: int, optional, default=100\n            Number of reservoir neurons. Only used if W is not passed.\n            If W is passed, n_reservoir gets overwritten with len(W).\n            Either n_reservoir or W must be passed.\n        W: np.ndarray of shape (n_reservoir, n_reservoir), optional, default=None\n            Reservoir weights matrix. If None, random weights are used (uniformly\n            distributed around 0, ie., in [-0.5, 0.5).\n            Be careful with the distribution of W values. Wrong W initialization\n            might drastically affect test performance (even with reasonable good\n            training fit).\n            Spectral radius will be adjusted in all cases.\n            Either n_reservoir or W must be passed.\n        spectral_radius: float, default=.99\n            Spectral radius of the reservoir weights matrix (W).\n            Spectral radius will be adjusted in all cases (also with user specified W).\n        W_in: np.ndarray of shape (n_reservoir, 1+n_inputs) (1-&gt;bias),\n            optional, default None.\n            Input weights matrix by which input signal is multiplied.\n            If None, random weights are used.\n        W_fb: np.ndarray of shape(n_reservoir, n_outputs), optional, default None.\n            Feedback weights matrix by which feedback is multiplied in case of feedback.\n        sparsity: float, optional, default=0\n            Proportion of the reservoir matrix weights forced to be zero.\n            Note that with default W (centered around 0), the actual sparsity will\n            be slightly more than the specified.\n            If W is passed, sparsity will be ignored.\n        noise: float, optional, default=0\n            Scaling factor of the (uniform) noise input added to neurons at each step.\n            This is used for regularization purposes and should typically be\n            very small, e.g. 0.0001 or 1e-5.\n        leak_rate: float, optional, default=1\n            Leaking rate applied to the neurons at each step.\n            Default is 1, which is no leaking. 0 would be total leakeage.\n        bias: int, float or np.ndarray, optional, default=1\n            Value of the bias neuron, injected at each time to the reservoir neurons.\n            If int or float, all neurons receive the same.\n            If np.ndarray is must be of length n_reservoir.\n        input_scaling: float or np.ndarray of length n_inputs, default=None\n            Scalar to multiply each input before feeding it to the network.\n            If float, all inputs get multiplied by same value.\n            If array, it must match n_inputs length (X.shape[1]), specifying the scaling\n            factor for each input.\n        input_shift: float or np.ndarray of length n_inputs, default=None\n            Scalar to add to each input before feeding it to the network.\n            If float, multiplied same value is added to all inputs.\n            If array, it must match n_inputs length (X.shape[1]), specifying the value\n            to add to each input.\n        feedback: bool, optional, default=False\n            If True, the reservoir also receives the outout signal as input.\n        activation: function (numba jitted), optional, default=tanh\n            Non-linear activation function applied to the neurons at each step.\n            For numba acceleration, it must be a jitted function.\n            Basic activation functions as tanh, sigmoid, relu are already available\n            in echoe.utils. Either use those or write a custom one decorated with\n            numba njit.\n        activation_out: function, optional, default=identity\n            Activation function applied to the outputs. In other words, it is assumed\n            that targets = f(outputs). So the output produced must be transformed.\n        fit_only_states: bool,default=False\n            If True, outgoing weights (W_out) are computed fitting only the reservoir\n            states. Inputs and bias are still use to drive reservoir activity, but\n            ignored for fitting W_out, both in the training and prediction phase.\n        regression_method: str, optional, default \"pinv\" (pseudoinverse).\n            Method to solve the linear regression to find out outgoing weights.\n            One of [\"pinv\", \"ridge\"].\n            If \"ridge\", ridge_* parameters will be used.\n        ridge_alpha: float, ndarray of shape (n_outputs,), default=1\n            Regularization coefficient used for Ridge regression.\n            Larger values specify stronger regularization.\n            If an array is passed, penalties are assumed to be specific to the targets.\n            Hence they must correspond in number.\n            Default is None to make sure one deliberately sets this since it is\n            a crucial parameter. See sklearn Ridge documentation for details.\n        ridge_fit_intercept: bool, optional, default=False\n            If True, intercept is fit in Ridge regression. Default False.\n            See sklearn Ridge documentation for details.\n        ridge_max_iter: int, default=None\n            Maximum number of iterations for conjugate gradient solver.\n            See sklearn Ridge documentation for details.\n        ridge_tol: float, default=1e-3\n            Precision of the solution.\n            See sklearn Ridge documentation for details.\n        ridge_solver: str, optional, default=\"auto\"\n            Solver to use in the Ridge regression.\n            One of [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"].\n            See sklearn Ridge documentation for details.\n        ridge_sample_weight: float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample.\n            If given a float, every sample will have the same weight.\n            See sklearn Ridge documentation for details.\n        n_transient: int, optional, default=0\n            Number of activity initial steps removed (not considered for training)\n            in order to avoid initial instabilities.\n            Default is 0, but this is something one definitely might want to tweak.\n        random_state : int, RandomState instance, default=None\n            The seed of the pseudo random number generator used to generate weight\n            matrices, to generate noise inyected to reservoir neurons (regularization)\n            and it is passed to the ridge solver in case regression_method=ridge.\n            From sklearn:\n              If int, random_state is the seed used by the random number generator;\n              If RandomState instance, random_state is the random number generator;\n              If None, the random number generator is the RandomState instance used\n              by `np.random`.\n        store_states_train: bool, optional, default=False\n            If True, time series series of reservoir neurons during training are stored\n            in the object attribute states_train_.\n        store_states_pred: bool, optional, default=False\n            If True, time series series of reservoir neurons during prediction are\n            stored in the object attribute states_pred_.\n\n    ### Attributes:\n        - W_out_ : array of shape (n_outputs, n_inputs + n_reservoir + 1)\n             Outgoing weights after fitting linear regression model to predict outputs.\n        - training_prediction_: array of shape (n_samples, n_outputs)\n             Predicted output on training data.\n        - states_train_: array of shape (n_samples, n_reservoir), default False.\n             If store_states_train is True, states matrix is stored for visualizing\n             reservoir neurons activity during training.\n        - states_pred_: array of shape (n_samples, n_reservoir), default False.\n             If store_states_pred is True, states matrix is stored for visualizing\n             reservoir neurons activity during prediction (test).\n    \"\"\"\n\n    def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"ESNRegressor\":\n        \"\"\"\n        Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later\n        prediction.\n        Bias is appended automatically to the inputs.\n\n        Arguments:\n            X: None or 2D np.ndarray of shape (n_samples, n_inputs)\n                Training input, i.e., X, the features.\n                If None, it is assumed that only the target sequence matters (outputs)\n                and simply a sequence of zeros will be fed in - matching the\n                len(outputs).\n                This is to be used in the case of generative mode.\n            y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs)\n                Training output, i.e., y, the target.\n\n        Returns\n            self: returns an instance of self.\n        \"\"\"\n        self._dtype_ = X.dtype\n        X, y = check_X_y(X, y, multi_output=True, dtype=self._dtype_)\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n        # Check y again (enforcing 2D for multiple outputs)\n        y = check_array(y, dtype=self._dtype_)\n\n        # Initialize matrices and random state\n        self.random_state_ = check_random_state(self.random_state)\n        self.n_inputs_ = X.shape[1]\n        self.n_reservoir_ = len(self.W) if self.W is not None else self.n_reservoir\n        self.n_outputs_ = y.shape[1]\n        self.W_in_ = self._init_incoming_weights()\n        self.W_ = self._init_reservoir_weights()\n        self.W_fb_ = self._init_feedback_weights()\n\n        check_model_params(self.__dict__)\n        X = self._scale_shift_inputs(X)\n\n        # Initialize reservoir model\n        self.reservoir_ = self._init_reservoir_neurons()\n\n        # Run \"neuronal activity\"\n        states = self.reservoir_.harvest_states(X, y, initial_state=None)\n\n        # Extend states matrix with inputs, except we only train based on states\n        full_states = states if self.fit_only_states else np.hstack((states, X))\n\n        # Solve for W_out using full states and outputs, excluding transient\n        self.W_out_ = self._solve_W_out(\n            full_states[self.n_transient :, :], y[self.n_transient :, :]\n        )\n        # Predict on training set (including the pass through the output nonlinearity)\n        self.training_prediction_ = self.activation_out(full_states @ self.W_out_.T)\n\n        # Store reservoir activity\n        if self.store_states_train:\n            self.states_train_ = states\n        return self\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict outputs according to inputs.\n        State/output is reinitialized to predict test outputs from\n        inputs as a typical predictive model. Since the reservoir states are\n        reinitialized, an initial transient, unstable phase will occur, so you\n        might want to cut off those steps to test performance (as done by the\n        parameter n_transient during training).\n\n        Arguments:\n            X: 2D np.ndarray of shape (n_samples, n_inputs)\n                Input, i.e., X, the features.\n\n        Returns:\n            y_pred: 2D np.ndarray of shape (n_samples, n_outputs)\n                Predicted outputs.\n        \"\"\"\n        check_is_fitted(self)\n        X = check_array(X, dtype=self._dtype_)\n\n        n_time_steps = X.shape[0]\n\n        # Scale and shift inputs\n        X = self._scale_shift_inputs(X)\n\n        # Initialize predictions\n        states = np.zeros((n_time_steps, self.n_reservoir_), dtype=self._dtype_)\n        y_pred = np.zeros((n_time_steps, self.n_outputs_), dtype=self._dtype_)\n\n        check_consistent_length(X, y_pred)  # sanity check\n\n        # Go through samples (steps) and predict for each of them\n        for t in range(1, n_time_steps):\n            states[t, :] = self.reservoir_.update_state(\n                state_t=states[t - 1, :],\n                X_t=X[t, :],\n                y_t=y_pred[t - 1, :],\n            )\n\n            if self.fit_only_states:\n                full_states = states[t, :]\n            else:\n                full_states = np.concatenate([states[t, :], X[t, :]])\n            # Predict\n            y_pred[t, :] = self.activation_out(self.W_out_ @ full_states)\n\n        # Store reservoir activity\n        if self.store_states_pred:\n            self.states_pred_ = states\n\n        return y_pred\n\n    def score(self, X: np.ndarray, y=np.ndarray, sample_weight=None) -&gt; float:\n        \"\"\"\n        R^2 (coefficient of determination) regression score function.\n\n        By default, the initial transient period (n_transient steps) is not considered\n        to compute the score - modify sample_weight to change that behaviour\n        (see below).\n\n        From sklearn:\n          Best possible score is 1.0 and it can be negative (because the model can be\n          arbitrarily bad).\n          A constant model that always predicts the expected value of y,\n          disregarding the input features, would get a R^2 score of 0.0.\n\n        Arguments:\n            X: 2D np.ndarray of shape (n_samples, n_inputs)\n                Test samples.\n            y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs)\n                Target sequence, true values of the outputs.\n            sample_weight: array-like of shape (n_samples,), default=None\n                Sample weights.\n                If None, the transient is left out.\n                To consider all steps or leave out a different transient, pass a\n                different sample_weight array with same length as outputs 1 dimension.\n                **Usage**\n                  &gt;&gt; n_steps_to_remove = 10\n                  &gt;&gt; weights = np.ones(y_true.shape[0])\n                  &gt;&gt; weights[: n_steps_to_remove] = 0\n                  &gt;&gt; score(X, y_true, sample_weight=weights)\n\n        Returns:\n            score: float\n                R2 score\n        \"\"\"\n        y_pred = self.predict(X)\n        # If no sample_weight passed, compute the score without considering transient\n        if sample_weight is None:\n            weights = np.ones(y.shape[0])\n            weights[: self.n_transient] = 0\n            return r2_score(y, y_pred, sample_weight=weights)\n\n        return r2_score(y, y_pred, sample_weight=sample_weight)\n</code></pre>"},{"location":"api/ESNRegressor/#echoes.ESNRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later prediction. Bias is appended automatically to the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>None or 2D np.ndarray of shape (n_samples, n_inputs) Training input, i.e., X, the features. If None, it is assumed that only the target sequence matters (outputs) and simply a sequence of zeros will be fed in - matching the len(outputs). This is to be used in the case of generative mode.</p> required <code>y</code> <code>ndarray</code> <p>2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Training output, i.e., y, the target.</p> required <p>Returns     self: returns an instance of self.</p> Source code in <code>echoes/esn/_regressor.py</code> <pre><code>def fit(self, X: np.ndarray, y: np.ndarray) -&gt; \"ESNRegressor\":\n    \"\"\"\n    Fit Echo State model, i.e., find outgoing weights matrix (W_out) for later\n    prediction.\n    Bias is appended automatically to the inputs.\n\n    Arguments:\n        X: None or 2D np.ndarray of shape (n_samples, n_inputs)\n            Training input, i.e., X, the features.\n            If None, it is assumed that only the target sequence matters (outputs)\n            and simply a sequence of zeros will be fed in - matching the\n            len(outputs).\n            This is to be used in the case of generative mode.\n        y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            Training output, i.e., y, the target.\n\n    Returns\n        self: returns an instance of self.\n    \"\"\"\n    self._dtype_ = X.dtype\n    X, y = check_X_y(X, y, multi_output=True, dtype=self._dtype_)\n    if y.ndim == 1:\n        y = y.reshape(-1, 1)\n    # Check y again (enforcing 2D for multiple outputs)\n    y = check_array(y, dtype=self._dtype_)\n\n    # Initialize matrices and random state\n    self.random_state_ = check_random_state(self.random_state)\n    self.n_inputs_ = X.shape[1]\n    self.n_reservoir_ = len(self.W) if self.W is not None else self.n_reservoir\n    self.n_outputs_ = y.shape[1]\n    self.W_in_ = self._init_incoming_weights()\n    self.W_ = self._init_reservoir_weights()\n    self.W_fb_ = self._init_feedback_weights()\n\n    check_model_params(self.__dict__)\n    X = self._scale_shift_inputs(X)\n\n    # Initialize reservoir model\n    self.reservoir_ = self._init_reservoir_neurons()\n\n    # Run \"neuronal activity\"\n    states = self.reservoir_.harvest_states(X, y, initial_state=None)\n\n    # Extend states matrix with inputs, except we only train based on states\n    full_states = states if self.fit_only_states else np.hstack((states, X))\n\n    # Solve for W_out using full states and outputs, excluding transient\n    self.W_out_ = self._solve_W_out(\n        full_states[self.n_transient :, :], y[self.n_transient :, :]\n    )\n    # Predict on training set (including the pass through the output nonlinearity)\n    self.training_prediction_ = self.activation_out(full_states @ self.W_out_.T)\n\n    # Store reservoir activity\n    if self.store_states_train:\n        self.states_train_ = states\n    return self\n</code></pre>"},{"location":"api/ESNRegressor/#echoes.ESNRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict outputs according to inputs. State/output is reinitialized to predict test outputs from inputs as a typical predictive model. Since the reservoir states are reinitialized, an initial transient, unstable phase will occur, so you might want to cut off those steps to test performance (as done by the parameter n_transient during training).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D np.ndarray of shape (n_samples, n_inputs) Input, i.e., X, the features.</p> required <p>Returns:</p> Name Type Description <code>y_pred</code> <code>ndarray</code> <p>2D np.ndarray of shape (n_samples, n_outputs) Predicted outputs.</p> Source code in <code>echoes/esn/_regressor.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict outputs according to inputs.\n    State/output is reinitialized to predict test outputs from\n    inputs as a typical predictive model. Since the reservoir states are\n    reinitialized, an initial transient, unstable phase will occur, so you\n    might want to cut off those steps to test performance (as done by the\n    parameter n_transient during training).\n\n    Arguments:\n        X: 2D np.ndarray of shape (n_samples, n_inputs)\n            Input, i.e., X, the features.\n\n    Returns:\n        y_pred: 2D np.ndarray of shape (n_samples, n_outputs)\n            Predicted outputs.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X, dtype=self._dtype_)\n\n    n_time_steps = X.shape[0]\n\n    # Scale and shift inputs\n    X = self._scale_shift_inputs(X)\n\n    # Initialize predictions\n    states = np.zeros((n_time_steps, self.n_reservoir_), dtype=self._dtype_)\n    y_pred = np.zeros((n_time_steps, self.n_outputs_), dtype=self._dtype_)\n\n    check_consistent_length(X, y_pred)  # sanity check\n\n    # Go through samples (steps) and predict for each of them\n    for t in range(1, n_time_steps):\n        states[t, :] = self.reservoir_.update_state(\n            state_t=states[t - 1, :],\n            X_t=X[t, :],\n            y_t=y_pred[t - 1, :],\n        )\n\n        if self.fit_only_states:\n            full_states = states[t, :]\n        else:\n            full_states = np.concatenate([states[t, :], X[t, :]])\n        # Predict\n        y_pred[t, :] = self.activation_out(self.W_out_ @ full_states)\n\n    # Store reservoir activity\n    if self.store_states_pred:\n        self.states_pred_ = states\n\n    return y_pred\n</code></pre>"},{"location":"api/ESNRegressor/#echoes.ESNRegressor.score","title":"<code>score(X, y=np.ndarray, sample_weight=None)</code>","text":"<p>R^2 (coefficient of determination) regression score function.</p> <p>By default, the initial transient period (n_transient steps) is not considered to compute the score - modify sample_weight to change that behaviour (see below).</p> From sklearn <p>Best possible score is 1.0 and it can be negative (because the model can be arbitrarily bad). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D np.ndarray of shape (n_samples, n_inputs) Test samples.</p> required <code>y</code> <p>2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs) Target sequence, true values of the outputs.</p> <code>ndarray</code> <code>sample_weight</code> <p>array-like of shape (n_samples,), default=None Sample weights. If None, the transient is left out. To consider all steps or leave out a different transient, pass a different sample_weight array with same length as outputs 1 dimension. Usage</p> <p>n_steps_to_remove = 10 weights = np.ones(y_true.shape[0]) weights[: n_steps_to_remove] = 0 score(X, y_true, sample_weight=weights)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>score</code> <code>float</code> <p>float R2 score</p> Source code in <code>echoes/esn/_regressor.py</code> <pre><code>def score(self, X: np.ndarray, y=np.ndarray, sample_weight=None) -&gt; float:\n    \"\"\"\n    R^2 (coefficient of determination) regression score function.\n\n    By default, the initial transient period (n_transient steps) is not considered\n    to compute the score - modify sample_weight to change that behaviour\n    (see below).\n\n    From sklearn:\n      Best possible score is 1.0 and it can be negative (because the model can be\n      arbitrarily bad).\n      A constant model that always predicts the expected value of y,\n      disregarding the input features, would get a R^2 score of 0.0.\n\n    Arguments:\n        X: 2D np.ndarray of shape (n_samples, n_inputs)\n            Test samples.\n        y: 2D np.ndarray of shape (n_samples,) or (n_samples, n_outputs)\n            Target sequence, true values of the outputs.\n        sample_weight: array-like of shape (n_samples,), default=None\n            Sample weights.\n            If None, the transient is left out.\n            To consider all steps or leave out a different transient, pass a\n            different sample_weight array with same length as outputs 1 dimension.\n            **Usage**\n              &gt;&gt; n_steps_to_remove = 10\n              &gt;&gt; weights = np.ones(y_true.shape[0])\n              &gt;&gt; weights[: n_steps_to_remove] = 0\n              &gt;&gt; score(X, y_true, sample_weight=weights)\n\n    Returns:\n        score: float\n            R2 score\n    \"\"\"\n    y_pred = self.predict(X)\n    # If no sample_weight passed, compute the score without considering transient\n    if sample_weight is None:\n        weights = np.ones(y.shape[0])\n        weights[: self.n_transient] = 0\n        return r2_score(y, y_pred, sample_weight=weights)\n\n    return r2_score(y, y_pred, sample_weight=sample_weight)\n</code></pre>"},{"location":"api/plotting/","title":"plotting","text":"<p>Plotting functions often needed. Not extremely well polished, rather a tool for quick visualization.</p> <p>Plotting functions related to the Memory Capacity task.</p>"},{"location":"api/plotting/#echoes.plotting._core.plot_predicted_ts","title":"<code>plot_predicted_ts(ts_true, ts_pred, start=None, end=None, ax=None, title='', figsize=(6, 2), legend=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ts_true</code> <code>ndarray | list | Series</code> <p>np.ndarray, List, pd.Series Target time series.</p> required <code>ts_pred</code> <code>ndarray | list | Series</code> <p>np.ndarray, List, pd.Series Predicted time series.</p> required <code>start</code> <code>int | None</code> <p>int, optional Plot will be timeseries[start: end].</p> <code>None</code> <code>end</code> <code>int | None</code> <p>int, optional Plot will be timeseries[start: end].</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>plt.Axes, optional Axes to plot on. If None, a new figure is created. Default None</p> <code>None</code> <code>title</code> <code>str</code> <p>str,optional Plot title.</p> <code>''</code> <code>figsize</code> <code>tuple</code> <p>tuple Figure size. Default (6, 2).</p> <code>(6, 2)</code> <code>legend</code> <code>bool</code> <p>bool If True, legend is added (\"target\", \"predicted\").</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ax</code> <p>matplotlib Axes Returns the Axes object with the plot drawn onto it.</p> Source code in <code>echoes/plotting/_core.py</code> <pre><code>def plot_predicted_ts(\n    ts_true: np.ndarray | list | pd.Series,\n    ts_pred: np.ndarray | list | pd.Series,\n    start: int | None = None,\n    end: int | None = None,\n    ax: plt.Axes | None = None,\n    title: str = \"\",\n    figsize: tuple = (6, 2),\n    legend: bool = True,\n):\n    \"\"\"\n    Arguments:\n        ts_true: np.ndarray, List, pd.Series\n            Target time series.\n        ts_pred: np.ndarray, List, pd.Series\n            Predicted time series.\n        start: int, optional\n            Plot will be timeseries[start: end].\n        end: int, optional\n            Plot will be timeseries[start: end].\n        ax: plt.Axes, optional\n            Axes to plot on. If None, a new figure is created.\n            Default None\n        title: str,optional\n            Plot title.\n        figsize: tuple\n            Figure size.\n            Default (6, 2).\n        legend: bool\n            If True, legend is added (\"target\", \"predicted\").\n\n    Returns:\n        ax: matplotlib Axes\n            Returns the Axes object with the plot drawn onto it.\n    \"\"\"\n    if isinstance(ts_true, pd.Series):\n        ts_true = ts_true.values\n    if isinstance(ts_pred, pd.Series):\n        ts_pred = ts_pred.values\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    ax.set_title(title)\n    ax.plot(ts_true[start:end], color=\"steelblue\", label=\"target\", linewidth=5.5)\n    ax.set_xlabel(\"time\")\n\n    ax.plot(\n        ts_pred[start:end],\n        linestyle=\"--\",\n        color=\"orange\",\n        linewidth=2,\n        label=\"prediction\",\n    )\n    ax.set_ylabel(\"output value\")\n    ax.set_xlabel(\"time\")\n\n    if legend:\n        ax.legend()\n    return ax\n</code></pre>"},{"location":"api/plotting/#echoes.plotting._core.plot_reservoir_activity","title":"<code>plot_reservoir_activity(esn, neurons, train=False, pred=True, start=None, end=None, figsize=(15, 9), **kwargs)</code>","text":"<p>Plot the activity, ie time series of states, of the reservoir neurons.</p> <p>Parameters:</p> Name Type Description Default <code>esn</code> <code>ESNRegressor | ESNGenerator</code> <p>ESNPredictive, ESNGenerative Instances of ESN after fitting and/or prediction.</p> required <code>neurons</code> <code>ndarray | list</code> <p>np.ndarray or List List of reservoir neurons indices whose time series will be plotted.</p> required <code>train</code> <code>bool</code> <p>bool, optional If True, the time series during training will be plotted. Either train or pred must be True, but only one of the two.</p> <code>False</code> <code>pred</code> <code>bool</code> <p>bool, optional If True, the time series during prediction will be plotted. Either train or pred must be True, but only one of the two.</p> <code>True</code> <code>start</code> <code>int | None</code> <p>int, optional Plot will be timeseries[start: end].</p> <code>None</code> <code>end</code> <code>int | None</code> <p>int, optional Plot will be timeseries[start: end].</p> <code>None</code> <code>suptitle</code> <p>str, optional Plot suptitle.</p> required <code>figsize</code> <code>tuple</code> <p>tuple Figure size. Default (15, 10).</p> <code>(15, 9)</code> <code>kwargs</code> <p>dict Plotting kwargs passed to plt.plot</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <p>plt.figure Figure object for fine tuning.</p> Source code in <code>echoes/plotting/_core.py</code> <pre><code>def plot_reservoir_activity(\n    esn: ESNRegressor | ESNGenerator,\n    neurons: np.ndarray | list,\n    train: bool = False,\n    pred: bool = True,\n    start: int | None = None,\n    end: int | None = None,\n    figsize: tuple = (15, 9),\n    **kwargs,\n):\n    \"\"\"\n    Plot the activity, ie time series of states, of the reservoir\n    neurons.\n\n    Arguments:\n        esn: ESNPredictive, ESNGenerative\n            Instances of ESN after fitting and/or prediction.\n        neurons: np.ndarray or List\n            List of reservoir neurons indices whose time series will be plotted.\n        train: bool, optional\n            If True, the time series during training will be plotted.\n            Either train or pred must be True, but only one of the two.\n        pred: bool, optional\n            If True, the time series during prediction will be plotted.\n            Either train or pred must be True, but only one of the two.\n        start: int, optional\n            Plot will be timeseries[start: end].\n        end: int, optional\n            Plot will be timeseries[start: end].\n        suptitle: str, optional\n            Plot suptitle.\n        figsize: tuple\n            Figure size.\n            Default (15, 10).\n        kwargs: dict\n            Plotting kwargs passed to plt.plot\n\n    Returns:\n        fig: plt.figure\n            Figure object for fine tuning.\n    \"\"\"\n    assert train or pred, \"either train or pred must be True\"\n    assert not (train and pred), \"only one of train or pred can be True\"\n\n    n_neurons = len(neurons)\n    # Grab time series to plot\n    ts = esn.states_pred_ if pred else esn.states_train_\n\n    # Plot test\n    fig, axes = plt.subplots(\n        nrows=int(np.ceil(n_neurons / 3)), ncols=3, figsize=figsize\n    )\n\n    if \"linewidth\" in kwargs:\n        linewidth = kwargs.pop(\"linewidht\")\n    else:\n        linewidth = 3\n    if \"color\" in kwargs:\n        color = kwargs.pop(\"color\")\n    else:\n        color = \".6\"\n\n    for neuron_idx, neuron in enumerate(neurons):\n        ax = axes.flat[neuron_idx]\n        ax.plot(ts[start:end, neuron], linewidth=linewidth, color=color, **kwargs)\n        ax.set_ylabel(\"state\")\n        ax.set_xlabel(\"time\")\n        ax.set_title(f\"reservoir neuron idx: {neuron}\")\n\n    # Delete unnecessary axes\n    if n_neurons % 3 == 1:\n        fig.delaxes(axes.flat[-1])\n        fig.delaxes(axes.flat[-2])\n    elif n_neurons % 3 == 2:\n        fig.delaxes(axes.flat[-1])\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/plotting/#echoes.plotting._core.set_mystyle","title":"<code>set_mystyle()</code>","text":"<p>Set context and a couple of defaults for nicer plots.</p> Source code in <code>echoes/plotting/_core.py</code> <pre><code>def set_mystyle():\n    \"\"\"Set context and a couple of defaults for nicer plots.\"\"\"\n    sns.set_theme(\n        context=\"paper\",\n        style=\"whitegrid\",\n        font_scale=1.4,\n        rc={\"grid.linestyle\": \"--\", \"grid.linewidth\": 0.8},\n    )\n</code></pre>"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_forgetting_curve","title":"<code>plot_forgetting_curve(lags, forgetting_curve, ax=None, **kwargs)</code>","text":"<p>Plot forgetting curve, ie, memory capacity (MC) vs lag.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>list | ndarray</code> <p>np.ndarray or List Sequence of lags used in the memory capacity task.</p> required <code>forgetting_curve</code> <code>ndarray</code> <p>np.ndarray Sequence of results from the memory task.</p> required <code>ax</code> <code>Axes</code> <p>plt.Axes, optional If given plot will use this axes.</p> <code>None</code> <code>kwargs</code> <p>mapping, optional Plotting args passed to ax.plot.</p> <code>{}</code> Source code in <code>echoes/plotting/_memory_capacity.py</code> <pre><code>def plot_forgetting_curve(\n    lags: list | np.ndarray,\n    forgetting_curve: np.ndarray,\n    ax: plt.Axes = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Plot forgetting curve, ie, memory capacity (MC) vs lag.\n\n    Arguments:\n        lags: np.ndarray or List\n            Sequence of lags used in the memory capacity task.\n        forgetting_curve: np.ndarray\n            Sequence of results from the memory task.\n        ax: plt.Axes, optional\n            If given plot will use this axes.\n        kwargs: mapping, optional\n            Plotting args passed to ax.plot.\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.plot(lags, forgetting_curve, **kwargs)\n    ax.set_xlabel(\"$k$\")\n    ax.set_ylabel(r\"$MC_k$\")\n</code></pre>"},{"location":"api/plotting/#echoes.plotting._memory_capacity.plot_mc_predicted_ts","title":"<code>plot_mc_predicted_ts(lags, outputs_true, outputs_pred, start=None, end=None)</code>","text":"<p>Plot true and predicted time series coming from memory capacity task for all lags.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>list | ndarray</code> <p>np.ndarray or List Delays to be evaluated (memory capacity). For example: np.arange(1, 31, 5).</p> required <code>ouputs_true</code> <p>np.ndarray of shape (len(ts), len(n_lags)) Target time series used for testing the model.</p> required <code>ouputs_pred</code> <p>np.ndarray of shape (len(ts), len(n_lags)) Predicted time series resulting from testing the model.</p> required <code>start/end</code> <p>int, optional Plot will we timeseries[start: end], to exclude transient.</p> required Source code in <code>echoes/plotting/_memory_capacity.py</code> <pre><code>def plot_mc_predicted_ts(\n    lags: list | np.ndarray,\n    outputs_true: np.ndarray,\n    outputs_pred: np.ndarray,\n    start: int | None = None,\n    end: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Plot true and predicted time series coming from memory capacity\n    task for all lags.\n\n    Arguments:\n        lags: np.ndarray or List\n            Delays to be evaluated (memory capacity).\n            For example: np.arange(1, 31, 5).\n        ouputs_true: np.ndarray of shape (len(ts), len(n_lags))\n            Target time series used for testing the model.\n        ouputs_pred: np.ndarray of shape (len(ts), len(n_lags))\n            Predicted time series resulting from testing the model.\n        start/end: int, optional\n            Plot will we timeseries[start: end], to exclude transient.\n    \"\"\"\n    assert (\n        outputs_true.shape == outputs_pred.shape\n    ), \"true and pred outputs must have same shape\"\n    assert (\n        len(lags) == outputs_true.shape[1]\n    ), \"second dimension of outputs must equal len(lags)\"\n\n    n_lags = len(lags)\n\n    # Plot test\n    fig, axes = plt.subplots(\n        nrows=int(np.ceil(n_lags / 2)), ncols=2, figsize=(18, 2.0 * n_lags)\n    )\n    for lag_idx, lag in enumerate(lags):\n        ax = axes.flat[lag_idx]\n\n        plot_predicted_ts(\n            outputs_true[:, lag_idx],\n            outputs_pred[:, lag_idx],\n            start=start,\n            end=end,\n            title=f\"lag = {lag}\",\n            ax=ax,\n            legend=False,\n        )\n\n    handles, labels = ax.get_legend_handles_labels()\n    fig.legend(\n        handles,\n        labels,\n        loc=\"upper center\",\n        fontsize=20,\n        ncol=2,\n        fancybox=True,\n        shadow=True,\n    )\n    if n_lags % 2 != 0:\n        fig.delaxes(axes.flat[-1])\n    fig.tight_layout()\n</code></pre>"},{"location":"api/utils/","title":"utils","text":"<p>Auxiliar functions</p>"},{"location":"api/utils/#echoes.utils.check_func_inverse","title":"<code>check_func_inverse(func, inv_func)</code>","text":"<p>check that func and inv_func are indeed inverse of each other</p> Source code in <code>echoes/utils.py</code> <pre><code>def check_func_inverse(func: Callable, inv_func: Callable) -&gt; None:\n    \"\"\"check that func and inv_func are indeed inverse of each other\"\"\"\n    x = np.linspace(-2, 2, 10)\n    y = func(x)\n    mismatch = np.where(inv_func(y) != x)[0]\n    assert np.isclose(\n        inv_func(y), x\n    ).all(), f\"function {inv_func.__name__} is not the inverse of {func.__name__}\"\n</code></pre>"},{"location":"api/utils/#echoes.utils.check_matrices_shapes","title":"<code>check_matrices_shapes(W_in, W, W_fb, n_inputs, n_reservoir, n_outputs, feedback)</code>","text":"<p>Check shapes of W, W_in, W_fb</p> Source code in <code>echoes/utils.py</code> <pre><code>def check_matrices_shapes(W_in, W, W_fb, n_inputs, n_reservoir, n_outputs, feedback):\n    \"\"\"Check shapes of W, W_in, W_fb\"\"\"\n    assert W.shape[0] == W.shape[1], \"W must be square\"\n    assert len(W) == n_reservoir, \"W does not match n_reservoir\"\n    assert W_in.shape[0] == n_reservoir, \"W_in first dimension must equal n_reservoir\"\n    assert W_in.shape[1] == n_inputs, \"W_in second dimension must equal n_inputs\"\n    if feedback:\n        assert W_fb is not None, \"W_fb must be specified if feedback=True\"\n    if W_fb is not None:\n        assert (\n            W_fb.shape[0] == n_reservoir\n        ), \"W_fb first dimension must equal n_reservoir\"\n        assert W_fb.shape[1] == n_outputs, \"W_fb second dimension must equal n_outputs\"\n</code></pre>"},{"location":"api/utils/#echoes.utils.check_model_params","title":"<code>check_model_params(params)</code>","text":"<p>check consistency of parameters, shapes, sensible warnings</p> Source code in <code>echoes/utils.py</code> <pre><code>def check_model_params(\n    params: Mapping,\n) -&gt; None:\n    \"\"\"check consistency of parameters, shapes, sensible warnings\"\"\"\n    W_in = params[\"W_in_\"]\n    W = params[\"W_\"]\n    W_fb = params[\"W_fb_\"]\n    n_reservoir = params[\"n_reservoir_\"]\n    n_inputs = params[\"n_inputs_\"]\n    n_outputs = params[\"n_outputs_\"]\n    feedback = params[\"feedback\"]\n    input_scaling = params[\"input_scaling\"]\n    input_shift = params[\"input_shift\"]\n\n    check_matrices_shapes(W_in, W, W_fb, n_inputs, n_reservoir, n_outputs, feedback)\n    check_sparsity(params[\"sparsity\"])\n    check_input_scaling(input_scaling, n_inputs)\n    check_input_shift(input_scaling, n_inputs)\n\n    # Warnings\n    if params[\"leak_rate\"] == 0:\n        warnings.warn(\n            \"leak_rate == 0 is total leakeage, you probably meant 1. See documentation.\"\n        )\n    if (\n        params[\"regression_method\"] != \"ridge\"\n        and params[\"ridge_sample_weight\"] is not None\n    ):\n        warnings.warn(\n            \"ridge_sample_weight will be ignored since regression_method is not ridge\"\n        )\n</code></pre>"},{"location":"api/utils/#echoes.utils.relu","title":"<code>relu(x)</code>","text":"<p>Numba jitted ReLu (rectified linear unit) function. Return ReLu(x)</p> Source code in <code>echoes/utils.py</code> <pre><code>@njit\ndef relu(x: float | np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Numba jitted ReLu (rectified linear unit) function. Return ReLu(x)\"\"\"\n    return np.maximum(0, x)\n</code></pre>"},{"location":"api/utils/#echoes.utils.set_spectral_radius","title":"<code>set_spectral_radius(matrix, target_radius)</code>","text":"<p>Return a copy of matrix with rescaled weights to match target spectral radius</p> Source code in <code>echoes/utils.py</code> <pre><code>def set_spectral_radius(matrix: np.ndarray, target_radius: float) -&gt; np.ndarray:\n    \"\"\"Return a copy of matrix with rescaled weights to match target spectral radius\"\"\"\n    matrix = matrix.copy()\n    current_radius = np.max(np.abs(np.linalg.eigvals(matrix)))\n    matrix *= target_radius / current_radius\n    return matrix\n</code></pre>"},{"location":"api/utils/#echoes.utils.sigmoid","title":"<code>sigmoid(x, a=1)</code>","text":"<p>Return  f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function.</p> Source code in <code>echoes/utils.py</code> <pre><code>@njit\ndef sigmoid(x: float | np.ndarray, a: float = 1) -&gt; float | np.ndarray:\n    \"\"\"\n    Return  f(x) = 1 / (1 + np.exp(-x * a)). Numba jitted sigmoid function.\n    \"\"\"\n    return 1 / (1 + np.exp(-x * a))\n</code></pre>"},{"location":"api/utils/#echoes.utils.tanh","title":"<code>tanh(x)</code>","text":"<p>Numba jitted tanh function. Return tanh(x)</p> Source code in <code>echoes/utils.py</code> <pre><code>@njit\ndef tanh(x: float | np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Numba jitted tanh function. Return tanh(x)\"\"\"\n    return np.tanh(x)\n</code></pre>"},{"location":"examples/plot_generator_mackeyglass17/","title":"ESNGenerator (Mackey-Glass)","text":"<p>ESNGenerator example</p> <p>This example shows how to use Echo State Networks as  pattern generator to produce a Mackey-Glass system.</p> <p>We try to predict the future steps of a chaotic time series.  For testing, we use the Echo State Network in \"generative mode\", which means, we do not have any input (only the bias) and the output  is fed back into the network for producing the next time step.</p> <p>Several parameter used in the example are arbitrary and even not so conventional (e.g., spectral radius &gt; 1), just  just for the sake of the example.  Other constellations produce satisfactory results too,  so feel free to play around with them.</p> <pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom echoes import ESNGenerator\nfrom echoes.datasets import load_mackeyglasst17\nfrom echoes.plotting import plot_predicted_ts, set_mystyle\n\nset_mystyle()  # optional, set aesthetics\n\n\n# Load and split data\nmackey_ts = load_mackeyglasst17()\nn_train_steps, n_test_steps = 2000, 2000\nn_total_steps = n_train_steps + n_test_steps\n\ny_train, y_test = train_test_split(\n    mackey_ts, \n    train_size=n_train_steps, \n    test_size=n_test_steps, \n    shuffle=False\n)\n\nesn = ESNGenerator(\n    n_steps=n_test_steps,\n    n_reservoir=200,\n    spectral_radius=1.25,\n    leak_rate=.4,\n    random_state=42,\n)\n# Fit the model. Inputs is None because we only have the target time series\nesn.fit(X=None, y=y_train)\n\ny_pred = esn.predict()\nprint(\"test r2 score\", r2_score(y_test, y_pred))\n\n# Plot training and test\nplt.figure(figsize=(22, 5))\nplt.plot(mackey_ts[: n_total_steps], 'steelblue', linewidth=5, label=\"target system\")\nplt.plot(esn.training_prediction_, color=\"y\", linewidth=1, label=\"training fit\")\nplt.plot(range(n_train_steps, n_total_steps), y_pred,'orange', label=\"ESNGenerator\")\nplt.ylabel(\"oscillator value\")\nplt.xlabel('time')\nlo, hi = plt.ylim()\nplt.vlines(n_train_steps, lo-.05, hi+.05, linestyles='--')\nplt.legend(fontsize='small')\n\n# Plot test alone\nplt.figure(figsize=(22, 5))\nplt.subplot(1, 4, (1, 3))\nplt.title(\"zoom into test\")\nplt.plot(y_test,\n         color=\"steelblue\",\n         label=\"target system\", \n         linewidth=5.5)\nplt.xlabel('time')\n\nplt.plot(y_pred, \n         linestyle='--',\n         color=\"orange\", \n         linewidth=2,\n         label=\"generative ESN\",)\nplt.ylabel(\"oscillator\")\nplt.xlabel('time')\nplt.legend(fontsize='small')\n\nplt.subplot(1, 4, 4)\nplt.title(r\"$W^{out}$ weights distribution\")\nplt.xlabel('weight')\nplt.ylabel('frequency')\nplt.hist(esn.W_out_.flat)\nplt.tight_layout();\n</code></pre> <pre><code>test r2 score 0.4041090675570501\n</code></pre> <p> </p>"},{"location":"examples/plot_regressor_sincos/","title":"ESNRegressor (sin-cos)","text":"<p>ESNRegressor example</p> <p>This example shows a minimal example of Echo State Network applied to a regression problem.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom echoes import ESNRegressor\nfrom echoes.plotting import set_mystyle\n\nset_mystyle()  # optional: set aesthetics\n\n\n# Prepare synthetic data \nx = np.linspace(0, 30*np.pi, 1000).reshape(-1,1)\ninputs = np.sin(x) + np.random.normal(scale=.1, size=x.shape)\noutputs = np.cos(x) + np.random.normal(scale=.1, size=x.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=.3, shuffle=False)\n\nesn = ESNRegressor(\n    spectral_radius=.95,\n    leak_rate=.4,\n    n_transient=100,\n    regression_method=\"pinv\",\n    random_state=42\n)\nesn.fit(X_train, y_train)\n\nprint(\"training r2 score: \", esn.score(X_train, y_train))\nprint(\"test r2 score: \", esn.score(X_test, y_test))\n\n# Get prediction for plotting\ny_pred = esn.predict(X_test)\n\nplt.figure(figsize=(15, 4))\nplt.subplot(1, 3, (1, 2))\nplt.plot(y_test[esn.n_transient:], label='target signal',\n         color=\"steelblue\", linewidth=5.5)\nplt.plot(y_pred[esn.n_transient:], label='predicted signal',\n         linestyle='--', color=\"orange\",  linewidth=2,)\nplt.ylabel(\"oscillator\")\nplt.xlabel('time')\nplt.legend(fontsize=(\"small\"), loc=2)\n\nplt.subplot(1, 3, 3)\nplt.title(r\"$W^{out}$ weights distribution\")\nplt.xlabel('weight')\nplt.ylabel('frequency')\nplt.hist(esn.W_out_.flat);\n</code></pre> <pre><code>training r2 score:  0.9840476267330117\ntest r2 score:  0.9733774695460478\n</code></pre> <p></p>"},{"location":"examples/plot_reservoir_activity/","title":"Visualization of reservoir neurons activity","text":"<p>This example shows how to access the time series of the  reservoir neurons activity during the prediction phase.</p> <p>We use the ESNGenerator model as a pattern generator that learns to predict future steps of the chaotic time series Mackey-Glass  Then we plot some randomly picked neurons from the reservoir.</p> <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom echoes import ESNGenerator\nfrom echoes.datasets import load_mackeyglasst17\nfrom echoes.plotting import (\n    set_mystyle, \n    plot_reservoir_activity, \n    plot_predicted_ts\n)\n\nset_mystyle()  # just aesthetics\n\n\n# Load data and define train/test length\ndata = load_mackeyglasst17().reshape(-1, 1)\nn_train_steps, n_test_steps = 2000, 2000\nn_total_steps = n_train_steps + n_test_steps\n\ny_train, y_test = train_test_split(\n    data, \n    train_size=n_train_steps, \n    test_size=n_test_steps, \n    shuffle=False\n)\n\n# Instantiate model\nesn = ESNGenerator(\n    n_reservoir=200,\n    n_steps=n_test_steps,\n    spectral_radius=1.25,\n    leak_rate=.4,\n    regression_method=\"pinv\",\n    store_states_pred=True,   # store states to plot later\n    random_state=42,\n).fit(None, y=y_train)\n\nprediction = esn.predict()\n</code></pre> <pre><code>ax = plot_predicted_ts(\n    y_test,\n    prediction,\n    end=500,\n    figsize=(12, 3)\n)\n# We can customize the plot\nax.set_title(\"My prediction\")\nax.legend(loc=1, fancybox=True, shadow=True)\n</code></pre> <p></p> <pre><code># Pick 9 random neurons to plot\nneurons_to_plot = sorted(np.random.randint(0, esn.n_reservoir, size=9))\n\n# This plots the activity and return the fig object for finetuning\nfig = plot_reservoir_activity(\n    esn,\n    neurons_to_plot,\n    pred=True,   # plot activity during prediction\n    end=500,\n    figsize=(12, 8),\n    color=\"tab:orange\",\n    alpha=.7\n)\n\n# Optional finetuning\nfor i, ax in enumerate(fig.axes):\n    if i % 2 == 0:\n        ax.set_title(\"$\\\\bf{CUSTOM-TITLE}$\")\n</code></pre> <p></p>"},{"location":"tutorial/","title":"What are Echo State Networks?","text":"<p>Coming soon!</p> <p>Meanwhile you can take a look at this article by H. Jaeger.</p>"}]}